"""–°–∫—Ä–µ–π–ø–µ—Ä –¥–ª—è –°–∞–º–æ–∫–∞—Ç."""

import asyncio
import re
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin, urlparse
from decimal import Decimal
from playwright.async_api import Page
from loguru import logger
from bs4 import BeautifulSoup

from .base import BaseScraper


class SamokratScraper(BaseScraper):
    """–°–∫—Ä–µ–π–ø–µ—Ä –¥–ª—è —Å–∞–π—Ç–∞ –°–∞–º–æ–∫–∞—Ç."""
    
    @property
    def shop_name(self) -> str:
        return "samokat"
    
    @property
    def base_url(self) -> str:
        return "https://samokat.ru"
    
    async def _set_location(self) -> None:
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≥–æ—Ä–æ–¥–∞ –∏ –∞–¥—Ä–µ—Å–∞ –≤ –°–∞–º–æ–∫–∞—Ç –ø–æ —Ä–∞–±–æ—á–µ–π –ª–æ–≥–∏–∫–µ."""
        page = await self.context.new_page()
        
        try:
            logger.info(f"Setting location to {self.config.city}, {self.config.address}")
            
            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            await page.goto(self.base_url, wait_until="networkidle")
            logger.info("‚úÖ Main page loaded")
            
            # –ñ–¥–µ–º 25 —Å–µ–∫—É–Ω–¥ –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –∫–æ–¥–µ
            await asyncio.sleep(25)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ
            await page.reload()
            await asyncio.sleep(2)
            logger.info("‚úÖ Page refreshed")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—á–Ω—ã–π XPath –∏–∑ —Ä–∞–±–æ—á–µ–≥–æ –∫–æ–¥–∞
            try:
                # –ö–ª–∏–∫–∞–µ–º –Ω–∞ –∞–¥—Ä–µ—Å (XPath –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞)
                address_click = await page.wait_for_selector("xpath=/html/body/div/section/aside[2]/div/div/div[1]/div/span[1]", timeout=10000)
                await address_click.click()
                logger.info("‚úÖ Address button clicked")
                await asyncio.sleep(3)
                
                # –ö–ª–∏–∫–∞–µ–º –Ω–∞ –∫–Ω–æ–ø–∫—É –∏–∑–º–µ–Ω–µ–Ω–∏—è –∞–¥—Ä–µ—Å–∞
                change_button = await page.wait_for_selector("xpath=/html/body/div[1]/div[2]/div[2]/nav/div/div[2]/div[82]/div", timeout=10000)
                change_button_el = await change_button.query_selector("button")
                await change_button_el.click()
                logger.info("‚úÖ Change address button clicked")
                await asyncio.sleep(4)
            
                # –í—ã–±–∏—Ä–∞–µ–º –≥–æ—Ä–æ–¥ –∏–∑ —Å–ø–∏—Å–∫–∞ (–ª–æ–≥–∏–∫–∞ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞)
                towns_container = await page.wait_for_selector(".AddressCreation_root__rnHIH", timeout=10000)
                suggest_container = await towns_container.query_selector(".Suggest_suggestItems__wOpnt")
                town_items = await suggest_container.query_selector_all(".Suggest_suggestItem__ZDojM")
                
                logger.info(f"Found {len(town_items)} towns")
                
                # –ò—â–µ–º –Ω—É–∂–Ω—ã–π –≥–æ—Ä–æ–¥
                for town_item in town_items:
                    town_text = await town_item.inner_text()
                    logger.debug(f"Town option: {town_text}")
                    if self.config.city in town_text:
                        await town_item.click()
                        logger.info(f"‚úÖ Selected city: {town_text}")
                        break
                
                await asyncio.sleep(5)
                
                # –í–≤–æ–¥–∏–º –∞–¥—Ä–µ—Å (–ª–æ–≥–∏–∫–∞ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞)
                address_suggest = await page.wait_for_selector(".AddressSuggest_addressSuggest__6Y9nV", timeout=10000)
                address_input = await address_suggest.query_selector("input")
                await address_input.type(self.config.address, delay=100)
                logger.info(f"‚úÖ Address typed: {self.config.address}")
                await asyncio.sleep(3)
                
                # –í—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –∏–∑ –ø–æ–¥—Å–∫–∞–∑–æ–∫
                suggest_item = await address_suggest.wait_for_selector(".Suggest_suggestItem__ZDojM", timeout=10000)
                await suggest_item.click()
                logger.info("‚úÖ Address suggestion selected")
                await asyncio.sleep(3)
                
                # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ–º –∞–¥—Ä–µ—Å –∫–Ω–æ–ø–∫–æ–π "–î–∞, –≤—Å—ë –≤–µ—Ä–Ω–æ"
                buttons = await page.query_selector_all("button")
                for button in buttons:
                    button_text = await button.inner_text()
                    if "–î–∞, –≤—Å—ë –≤–µ—Ä–Ω–æ" in button_text:
                        await button.click()
                        logger.info("‚úÖ Address confirmed")
                        break
                
            except Exception as e:
                logger.warning(f"Could not set location using exact XPath: {e}")
                logger.warning("Address selection failed - continuing anyway")
            
            await asyncio.sleep(3)
            logger.info("Location setup completed")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–µ—Å—Å–∏–∏ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            try:
                await page.context.storage_state(path="samokat_session.json")
                logger.info("‚úÖ Session state saved to samokat_session.json")
            except Exception as e:
                logger.warning(f"Could not save session state: {e}")
            
        except Exception as e:
            logger.warning(f"Could not set location automatically: {e}")
        finally:
            await page.close()
    
    async def _scrape_items(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã."""
        
        # –ü–†–ï–î–í–ê–†–ò–¢–ï–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –î–û–°–¢–£–ü–ù–û–°–¢–ò –°–ê–ú–û–ö–ê–¢–ê
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –°–∞–º–æ–∫–∞—Ç–∞...")
        test_page = await self.context.new_page()
        
        try:
            await test_page.goto("https://samokat.ru", wait_until="domcontentloaded", timeout=30000)
            title = await test_page.title()
            content = await test_page.content()
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç—É—Å –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥
            if any(phrase in content.lower() for phrase in [
                "–º—ã —Å–ª–æ–º–∞–ª–∏—Å—å", "—Å–ª–æ–º–∞–ª–∏—Å—å", "–æ—à–∏–±–∫–∞", "–Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω",
                "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä–∞–±–æ—Ç—ã", "–ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ", "–≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
            ]) or "—Å–ª–æ–º–∞–ª–∏—Å—å" in title.lower():
                
                logger.warning("‚ö†Ô∏è –°–ê–ú–û–ö–ê–¢: –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥")
                logger.info("üìÑ –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: " + title)
            
            logger.info("‚úÖ –°–∞–º–æ–∫–∞—Ç –¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –°–∞–º–æ–∫–∞—Ç–∞: {e}")
            await test_page.close()
            return []
        finally:
            await test_page.close()
        
        all_items = []
        
        # –ü—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã –°–∞–º–æ–∫–∞—Ç
        category_urls = [
            "https://samokat.ru/category/vsya-gotovaya-eda-13",
            "https://samokat.ru/category/gotovaya-eda-25", 
            "https://samokat.ru/category/supy",
            "https://samokat.ru/category/salaty-i-zakuski",
            "https://samokat.ru/category/chto-na-zavtrak",
            "https://samokat.ru/category/gotovaya-eda-i-vypechka-6",
            "https://samokat.ru/category/stritfud-1",
            "https://samokat.ru/category/pochti-gotovo",
            "https://samokat.ru/category/bolshie-portsii"
        ]
        
        for category_url in category_urls:
            try:
                category_name = category_url.split('/')[-1]
                logger.info(f"Scraping category URL: {category_url}")
                category_items = await self._scrape_category_url(category_url, category_name)
                all_items.extend(category_items)
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"Failed to scrape category {category_url}: {e}")
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
        seen_urls = set()
        unique_items = []
        for item in all_items:
            if item.get('url') not in seen_urls:
                seen_urls.add(item.get('url'))
                unique_items.append(item)
        
        logger.info(f"Found {len(unique_items)} unique items from {len(all_items)} total")
        return unique_items
    
    async def _scrape_category_url(self, category_url: str, category_name: str) -> List[Dict[str, Any]]:
        """–°–∫—Ä–µ–π–ø–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ –ø—Ä—è–º–æ–º—É URL –∫–∞—Ç–µ–≥–æ—Ä–∏–∏."""
        page = await self.context.new_page()
        items = []
        
        try:
            logger.info(f"üåê Loading category URL: {category_url}")
            
            await page.goto(category_url, wait_until="networkidle", timeout=60000)
            logger.info(f"‚úÖ Page loaded successfully")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            title = await page.title()
            logger.info(f"üìÑ Page title: {title}")
            
            # –ü–†–û–í–ï–†–ö–ê –ù–ê –¢–ï–•–ù–ò–ß–ï–°–ö–£–Æ –û–®–ò–ë–ö–£ –°–ê–ú–û–ö–ê–¢–ê
            page_content = await page.content()
            
            if any(phrase in page_content.lower() for phrase in [
                "–º—ã —Å–ª–æ–º–∞–ª–∏—Å—å", "—Å–ª–æ–º–∞–ª–∏—Å—å", "–æ—à–∏–±–∫–∞", "–Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω", 
                "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä–∞–±–æ—Ç—ã", "–ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ"
            ]):
                logger.warning("‚ö†Ô∏è –°–ê–ú–û–ö–ê–¢: –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ —Å–∞–π—Ç–µ, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥")
            
            if "—Å–ª–æ–º–∞–ª–∏—Å—å" in title.lower() or "–æ—à–∏–±–∫–∞" in title.lower():
                logger.warning("‚ö†Ô∏è –°–ê–ú–û–ö–ê–¢: –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥")
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ JavaScript –∏ –ø–æ—è–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            await asyncio.sleep(10)
            logger.info(f"‚è±Ô∏è Waited 10 seconds for JavaScript to load")
            
            # –ñ–¥–µ–º –ø–æ—è–≤–ª–µ–Ω–∏—è –∫–∞—Ä—Ç–æ—á–µ–∫ —Ç–æ–≤–∞—Ä–æ–≤ —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
            logger.info(f"üîç Waiting for product cards with content to appear...")
            
            # –ñ–¥–µ–º –Ω–µ –ø—Ä–æ—Å—Ç–æ —ç–ª–µ–º–µ–Ω—Ç—ã, –∞ —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º
            content_loaded = False
            for attempt in range(10):
                try:
                    # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º
                    # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ —Å –ª—é–±—ã–º —Ç–µ–∫—Å—Ç–æ–º
                    all_cards = await page.locator("[class*='Card']").all()
                    cards_with_text = []
                    for card in all_cards:
                        try:
                            text = await card.inner_text()
                            if text and len(text.strip()) > 3:
                                cards_with_text.append(card)
                        except:
                            continue
                    if cards_with_text:
                        logger.info(f"‚úÖ Found {len(cards_with_text)} cards with content")
                        content_loaded = True
                        break
                    else:
                        logger.debug(f"   Attempt {attempt + 1}: waiting for content...")
                        await asyncio.sleep(2)
                except Exception as e:
                    logger.debug(f"   Attempt {attempt + 1}: error - {e}")
                    await asyncio.sleep(2)
            
            if not content_loaded:
                logger.warning(f"‚ö†Ô∏è No cards with content appeared after 20s")
            
            # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤
            await self._smart_scroll_to_load_all(page)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–≤–∞—Ä—ã —Å —Ä–∞–±–æ—á–∏–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏ –¥–ª—è –°–∞–º–æ–∫–∞—Ç
            product_selectors = [
                # –†–∞–±–æ—á–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –∏–∑ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                '[class*="CatalogTreeSectionCard"]',
                '[class*="Card"]',
                '[class*="Product"]',
                '[class*="product"]',
                'a[href*="/product/"]',
                'a[href*="/goods/"]',
                '[data-testid="product-card"]',
                '.product-card',
                '.catalog-item',
                '.goods-tile'
            ]
            
            products = []
            logger.info(f"üîç Testing product selectors for Samokat...")
            
            for selector in product_selectors:
                try:
                    locator = page.locator(selector)
                    count = await locator.count()
                    logger.info(f"   {selector}: {count} elements")
                    if count > len(products):
                        products = await locator.all()
                        logger.info(f"‚úÖ Best selector so far: {selector} with {len(products)} products")
                except Exception as e:
                    logger.warning(f"   {selector}: ERROR - {e}")
            
            if not products:
                logger.warning(f"‚ùå No products found with standard selectors")
                
                # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                page_content = await page.content()
                logger.info(f"üìÑ Page content length: {len(page_content)} characters")
                
                if '—Ç–æ–≤–∞—Ä' in page_content.lower():
                    logger.info(f"‚úÖ Page contains word '—Ç–æ–≤–∞—Ä'")
                else:
                    logger.warning(f"‚ùå Page does not contain word '—Ç–æ–≤–∞—Ä'")
                
                if 'product' in page_content.lower():
                    logger.info(f"‚úÖ Page contains word 'product'")
                else:
                    logger.warning(f"‚ùå Page does not contain word 'product'")
                
                # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ª—é–±—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã
                logger.info(f"üîç Looking for any product links...")
                link_selectors = ['a[href*="/product/"]', 'a[href*="/goods/"]', 'a[href*="/item/"]', 'a[href*="/catalog/"]']
                
                for link_selector in link_selectors:
                    try:
                        links = await page.query_selector_all(link_selector)
                        logger.info(f"   {link_selector}: {len(links)} links")
                        if links:
                            products = links[:20]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 20 –¥–ª—è —Ç–µ—Å—Ç–∞
                            break
                    except Exception as e:
                        logger.warning(f"   {link_selector}: ERROR - {e}")
                
                if not products:
                    logger.error(f"‚ùå No product links found at all!")
                    return items
            
            logger.info(f"Processing {len(products)} products from category {category_name}")
            
            for product in products:
                try:
                    item_data = await self._extract_product_data(product, page, category_name)
                    if item_data:
                        items.append(item_data)
                except Exception as e:
                    logger.warning(f"Failed to extract product data: {e}")
            
        except Exception as e:
            logger.error(f"Failed to scrape category {category_url}: {e}")
        finally:
            await page.close()
        
        return items
    
    async def _scrape_category(self, category: str) -> List[Dict[str, Any]]:
        """–°–∫—Ä–µ–π–ø–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏."""
        page = await self.context.new_page()
        items = []
        
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º URL –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            category_url = f"{self.base_url}/catalog/{category}"
            logger.debug(f"Loading category URL: {category_url}")
            
            await page.goto(category_url, wait_until="networkidle")
            await asyncio.sleep(3)
            
            # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤
            await self._scroll_to_load_all(page)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–≤–∞—Ä—ã
            product_selectors = [
                # –†–∞–±–æ—á–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –∏–∑ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                '[class*="CatalogTreeSectionCard"]',
                '[class*="Card"]',
                '[data-testid="product-card"]',
                '.product-card',
                '.catalog-item',
                '[class*="product"]',
                '.goods-tile'
            ]
            
            products = []
            for selector in product_selectors:
                try:
                    products = await page.query_selector_all(selector)
                    if products:
                        logger.debug(f"Found {len(products)} products with selector: {selector}")
                        break
                except:
                    continue
            
            if not products:
                logger.warning(f"No products found in category {category}")
                return items
            
            logger.info(f"Processing {len(products)} products from category {category}")
            
            for product in products:
                try:
                    item_data = await self._extract_product_data(product, page, category)
                    if item_data:
                        items.append(item_data)
                except Exception as e:
                    logger.warning(f"Failed to extract product data: {e}")
            
        except Exception as e:
            logger.error(f"Failed to scrape category {category}: {e}")
        finally:
            await page.close()
        
        return items
    
    async def _extract_product_data(self, product_element, page: Page, category: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–∞ –∏–∑ –∫–∞—Ä—Ç–æ—á–∫–∏."""
        try:
            # –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ - —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –°–∞–º–æ–∫–∞—Ç
            name_selectors = [
                '[data-testid="product-name"]',
                '.product-name',
                '.product-title', 
                '.goods-tile__title',
                '[class*="title"]',
                '[class*="Title"]',
                '[class*="name"]',
                '[class*="Name"]',
                'h1', 'h2', 'h3', 'h4', 'h5',
                'a', # –ò–Ω–æ–≥–¥–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤ —Å—Å—ã–ª–∫–µ
                'span'
            ]
            
            name = None
            logger.debug(f"üè∑Ô∏è Looking for product name...")
            
            # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –≤–µ—Å—å —Ç–µ–∫—Å—Ç —ç–ª–µ–º–µ–Ω—Ç–∞
            try:
                all_text = await product_element.inner_text()
                logger.debug(f"üìù Full element text: {all_text[:100]}...")
            except:
                all_text = ""
            
            for selector in name_selectors:
                try:
                    locator = product_element.locator(selector)
                    if await locator.count() > 0:
                        name = await locator.first.inner_text()
                        if name and name.strip():
                            logger.debug(f"‚úÖ Found name with {selector}: {name[:30]}...")
                            break
                except Exception as e:
                    logger.debug(f"   {selector}: failed - {e}")
            
            # –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø–æ–ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ—Å—å —Ç–µ–∫—Å—Ç —ç–ª–µ–º–µ–Ω—Ç–∞
            if not name and all_text:
                # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –Ω–∞–∑–≤–∞–Ω–∏–µ
                clean_text = all_text.strip().split('\n')[0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É
                if len(clean_text) > 3 and not clean_text.isdigit():
                    name = clean_text
                    logger.debug(f"‚úÖ Using full element text as name: {name[:30]}...")
            
            if not name:
                logger.warning(f"‚ùå No name found for product")
                logger.debug(f"   Full element text was: {all_text[:200]}...")
                return None
            
            # –¶–µ–Ω–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª–∞—Å—Å—ã –∏–∑ —Ä–∞–±–æ—á–µ–≥–æ –∫–æ–¥–∞
            price_selectors = [
                '.ProductCardActions_text__3Uohy',  # –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞
                '.ProductCardActions_oldPrice__d7vDY',  # –°—Ç–∞—Ä–∞—è —Ü–µ–Ω–∞
                '[data-testid="product-price"]',
                '.product-price',
                '.price',
                '[class*="price"]',
                '[class*="Price"]',
                '[class*="ProductCard"]'
            ]
            
            price = None
            logger.debug(f"üí∞ Looking for product price...")
            
            for selector in price_selectors:
                try:
                    locator = product_element.locator(selector)
                    if await locator.count() > 0:
                        price_text = await locator.first.inner_text()
                        price = self._extract_price(price_text)
                        if price:
                            logger.debug(f"‚úÖ Found price with {selector}: {price}")
                            break
                        else:
                            logger.debug(f"   {selector}: found element but no price in text: {price_text}")
                except Exception as e:
                    logger.debug(f"   {selector}: failed - {e}")
            
            # –ï—Å–ª–∏ —Ü–µ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—â–µ–º –≤ –æ–±—â–µ–º —Ç–µ–∫—Å—Ç–µ —ç–ª–µ–º–µ–Ω—Ç–∞
            if not price and all_text:
                price = self._extract_price(all_text)
                if price:
                    logger.debug(f"‚úÖ Found price in full text: {price}")
            
            if not price:
                logger.warning(f"‚ö†Ô∏è No price found for product: {name} (will use 0)")
                logger.debug(f"   Full element text was: {all_text[:200]}...")
                price = Decimal("0")  # –ù–µ –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º —Ç–æ–≤–∞—Ä, —Å—Ç–∞–≤–∏–º 0
            
            # –°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–æ–≤–∞—Ä
            link_selectors = [
                'a[href]',
                '[data-testid="product-link"] a',
                '.product-link'
            ]
            
            url = None
            for selector in link_selectors:
                try:
                    locator = product_element.locator(selector)
                    if await locator.count() > 0:
                        href = await locator.first.get_attribute('href')
                        if href:
                            url = urljoin(self.base_url, href)
                            logger.debug(f"‚úÖ Found URL with {selector}: {url[:50]}...")
                            break
                except Exception as e:
                    logger.debug(f"   {selector}: failed - {e}")
            
            if not url:
                logger.warning(f"‚ö†Ô∏è No URL found for product: {name} (will generate)")
                url = f"{self.base_url}/product/unknown_{hash(name)}"
            
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_selectors = [
                'img[src]',
                '[data-testid="product-image"] img',
                '.product-image img'
            ]
            
            photo_url = None
            for selector in image_selectors:
                try:
                    img_element = await product_element.query_selector(selector)
                    if img_element:
                        src = await img_element.get_attribute('src')
                        if src:
                            photo_url = urljoin(self.base_url, src)
                            break
                except:
                    continue
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –≤–µ—Å –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
            portion_g = self._extract_weight_from_text(name)
            if not portion_g:
                # –ò—â–µ–º –≤–µ—Å –≤ –¥—Ä—É–≥–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö –∫–∞—Ä—Ç–æ—á–∫–∏
                weight_text = await self._extract_text_from_element(product_element, [
                    '[class*="weight"]', '[class*="Weight"]', 
                    '[class*="gram"]', '[class*="Gram"]',
                    '.weight', '.portion'
                ])
                if weight_text:
                    portion_g = self._extract_weight_from_text(weight_text)
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤–∏–¥–∏–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            all_text = await product_element.inner_text() if product_element else ""
            
            # –ò—â–µ–º —Ç–µ–≥–∏/–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ç–µ–∫—Å—Ç–µ
            tags = self._extract_tags_from_text(all_text)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID –∏–∑ URL
            native_id = self._extract_id_from_url(url)
            
            return {
                'native_id': native_id,
                'name': name.strip(),
                'category': category,
                'price': price,
                'url': url,
                'photo_url': photo_url,
                'portion_g': portion_g,
                'tags': tags,
                'composition': None,
                'kcal_100g': None,
                'protein_100g': None,
                'fat_100g': None,
                'carb_100g': None
            }
            
        except Exception as e:
            logger.warning(f"Failed to extract product data: {e}")
            return None
    
    async def _enrich_item_details(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–∞ –ø–µ—Ä–µ—Ö–æ–¥–æ–º –≤ –∫–∞—Ä—Ç–æ—á–∫—É."""
        page = await self.context.new_page()
        
        try:
            logger.debug(f"Enriching item: {item_data.get('name')}")
            
            await page.goto(item_data['url'], wait_until="networkidle")
            await asyncio.sleep(2)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            
            # –°–æ—Å—Ç–∞–≤/–∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã
            composition_selectors = [
                '[data-testid="composition"]',
                '.composition',
                '.ingredients',
                '.product-composition',
                '*:has-text("–°–æ—Å—Ç–∞–≤")',
                '*:has-text("–ò–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã")'
            ]
            
            composition = await self._extract_text_by_selectors(page, composition_selectors)
            
            # –ü–∏—â–µ–≤–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å
            nutrition_data = await self._extract_nutrition_info(page)
            
            # –í–µ—Å –ø–æ—Ä—Ü–∏–∏
            portion_g = await self._extract_portion_weight(page)
            
            # –¢–µ–≥–∏/–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
            tags = await self._extract_tags(page)
            
            # –ë—Ä–µ–Ω–¥
            brand_selectors = [
                '[data-testid="brand"]',
                '.brand',
                '.manufacturer',
                '*:has-text("–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å")'
            ]
            brand = await self._extract_text_by_selectors(page, brand_selectors)
            
            # –®—Ç—Ä–∏—Ö–∫–æ–¥
            barcode_selectors = [
                '[data-testid="barcode"]',
                '.barcode',
                '*:has-text("–®—Ç—Ä–∏—Ö–∫–æ–¥")'
            ]
            barcode = await self._extract_text_by_selectors(page, barcode_selectors)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
            item_data.update({
                'composition': composition,
                'portion_g': portion_g,
                'tags': tags,
                'brand': brand,
                'barcode': barcode,
                **nutrition_data
            })
            
        except Exception as e:
            logger.warning(f"Failed to enrich item {item_data.get('url')}: {e}")
        finally:
            await page.close()
        
        return item_data
    
    async def _extract_nutrition_info(self, page: Page) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–∏—â–µ–≤–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏."""
        nutrition_data = {}
        
        # –ò—â–µ–º –±–ª–æ–∫ —Å –ø–∏—â–µ–≤–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç—å—é
        nutrition_selectors = [
            '[data-testid="nutrition"]',
            '.nutrition',
            '.nutritional-value',
            '.energy-value',
            '*:has-text("–ü–∏—â–µ–≤–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å")',
            '*:has-text("–≠–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å")'
        ]
        
        nutrition_block = None
        for selector in nutrition_selectors:
            try:
                nutrition_block = await page.query_selector(selector)
                if nutrition_block:
                    break
            except:
                continue
        
        if nutrition_block:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –±–ª–æ–∫–∞
            nutrition_text = await nutrition_block.inner_text()
            
            # –ü–∞—Ä—Å–∏–º –∫–∞–ª–æ—Ä–∏–∏
            kcal_match = re.search(r'(\d+(?:[.,]\d+)?)\s*(?:–∫–∫–∞–ª|kcal)', nutrition_text, re.IGNORECASE)
            if kcal_match:
                nutrition_data['kcal_100g'] = Decimal(kcal_match.group(1).replace(',', '.'))
            
            # –ü–∞—Ä—Å–∏–º –±–µ–ª–∫–∏
            protein_match = re.search(r'–±–µ–ª–∫[–∏|–∞].*?(\d+(?:[.,]\d+)?)', nutrition_text, re.IGNORECASE)
            if protein_match:
                nutrition_data['protein_100g'] = Decimal(protein_match.group(1).replace(',', '.'))
            
            # –ü–∞—Ä—Å–∏–º –∂–∏—Ä—ã
            fat_match = re.search(r'–∂–∏—Ä[—ã|–∞].*?(\d+(?:[.,]\d+)?)', nutrition_text, re.IGNORECASE)
            if fat_match:
                nutrition_data['fat_100g'] = Decimal(fat_match.group(1).replace(',', '.'))
            
            # –ü–∞—Ä—Å–∏–º —É–≥–ª–µ–≤–æ–¥—ã
            carb_match = re.search(r'—É–≥–ª–µ–≤–æ–¥[—ã|–∞].*?(\d+(?:[.,]\d+)?)', nutrition_text, re.IGNORECASE)
            if carb_match:
                nutrition_data['carb_100g'] = Decimal(carb_match.group(1).replace(',', '.'))
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏—â–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        nutrient_patterns = {
            'kcal_100g': [r'(\d+(?:[.,]\d+)?)\s*(?:–∫–∫–∞–ª|kcal)', '–∫–∞–ª–æ—Ä–∏–∏', '—ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å'],
            'protein_100g': [r'(\d+(?:[.,]\d+)?)\s*–≥.*–±–µ–ª–∫', '–±–µ–ª–∫–∏', 'protein'],
            'fat_100g': [r'(\d+(?:[.,]\d+)?)\s*–≥.*–∂–∏—Ä', '–∂–∏—Ä—ã', 'fat'],
            'carb_100g': [r'(\d+(?:[.,]\d+)?)\s*–≥.*—É–≥–ª–µ–≤–æ–¥', '—É–≥–ª–µ–≤–æ–¥—ã', 'carbohydrates']
        }
        
        page_content = await page.content()
        
        for nutrient, patterns in nutrient_patterns.items():
            if nutrient not in nutrition_data:
                for pattern in patterns:
                    if isinstance(pattern, str):
                        # –ò—â–µ–º –ø–æ —Ç–µ–∫—Å—Ç—É
                        elements = await page.query_selector_all(f'*:has-text("{pattern}")')
                        for element in elements:
                            try:
                                text = await element.inner_text()
                                number_match = re.search(r'(\d+(?:[.,]\d+)?)', text)
                                if number_match:
                                    nutrition_data[nutrient] = Decimal(number_match.group(1).replace(',', '.'))
                                    break
                            except:
                                continue
                    else:
                        # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ
                        match = re.search(pattern, page_content, re.IGNORECASE)
                        if match:
                            nutrition_data[nutrient] = Decimal(match.group(1).replace(',', '.'))
                            break
        
        return nutrition_data
    
    async def _extract_portion_weight(self, page: Page) -> Optional[Decimal]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–µ—Å–∞ –ø–æ—Ä—Ü–∏–∏."""
        weight_selectors = [
            '[data-testid="weight"]',
            '.weight',
            '.portion-weight',
            '*:has-text("–í–µ—Å")',
            '*:has-text("–ú–∞—Å—Å–∞")'
        ]
        
        weight_text = await self._extract_text_by_selectors(page, weight_selectors)
        
        if weight_text:
            # –ò—â–µ–º –≤–µ—Å –≤ –≥—Ä–∞–º–º–∞—Ö
            weight_match = re.search(r'(\d+(?:[.,]\d+)?)\s*–≥', weight_text)
            if weight_match:
                return Decimal(weight_match.group(1).replace(',', '.'))
        
        # –ò—â–µ–º –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —Ç–æ–≤–∞—Ä–∞
        try:
            title = await page.title()
            weight_match = re.search(r'(\d+)\s*–≥', title)
            if weight_match:
                return Decimal(weight_match.group(1))
        except:
            pass
        
        return None
    
    async def _extract_tags(self, page: Page) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ —Ç–æ–≤–∞—Ä–∞."""
        tags = []
        
        tag_selectors = [
            '[data-testid="tags"]',
            '.tags',
            '.labels',
            '.badges',
            '.product-labels'
        ]
        
        for selector in tag_selectors:
            try:
                tag_elements = await page.query_selector_all(f'{selector} *')
                for element in tag_elements:
                    tag_text = await element.inner_text()
                    if tag_text and len(tag_text.strip()) > 1:
                        tags.append(tag_text.strip().lower())
            except:
                continue
        
        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
        try:
            page_content = await page.content()
            keywords = ['–æ—Å—Ç—Ä–æ–µ', '–æ—Å—Ç—Ä—ã–π', '–≤–µ–≥–µ—Ç–∞—Ä–∏–∞–Ω—Å–∫–∏–π', '–≤–µ–≥–∞–Ω', '–ø–ø', '–¥–∏–µ—Ç–∏—á–µ—Å–∫–∏–π', '–±–µ–∑ –≥–ª—é—Ç–µ–Ω–∞', '–±–µ–∑ –ª–∞–∫—Ç–æ–∑—ã']
            for keyword in keywords:
                if keyword in page_content.lower():
                    tags.append(keyword)
        except:
            pass
        
        return list(set(tags))  # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    
    async def _smart_scroll_to_load_all(self, page: Page) -> None:
        """–£–º–Ω–∞—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∞ —Å –æ–∂–∏–¥–∞–Ω–∏–µ–º –ø–æ—è–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –∫–∞—Ä—Ç–æ—á–µ–∫."""
        logger.info("üîÑ Smart scrolling to load all products...")
        
        prev_count = 0
        stable_rounds = 0
        max_rounds = 20
        
        for round_num in range(max_rounds):
            # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –≤–Ω–∏–∑
            await page.mouse.wheel(0, 4000)
            await asyncio.sleep(1)
            
            # –°—á–∏—Ç–∞–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏
            try:
                grid = page.locator("[class*='Card'], [class*='Product'], .product-card, .goods-tile")
                current_count = await grid.count()
                
                logger.debug(f"   Round {round_num + 1}: {current_count} cards")
                
                if current_count > prev_count:
                    prev_count = current_count
                    stable_rounds = 0
                    logger.debug(f"   ‚úÖ New cards loaded: {current_count}")
                else:
                    stable_rounds += 1
                    logger.debug(f"   ‚è∏Ô∏è No new cards: stable round {stable_rounds}")
                
                # –ï—Å–ª–∏ 3 —Ä–∞—É–Ω–¥–∞ –±–µ–∑ –Ω–æ–≤—ã—Ö –∫–∞—Ä—Ç–æ—á–µ–∫ - –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
                if stable_rounds >= 3:
                    logger.info(f"‚úÖ Scrolling completed: {current_count} cards loaded")
                    break
                    
            except Exception as e:
                logger.warning(f"Error during scroll round {round_num + 1}: {e}")
                break
        
        logger.info(f"üèÅ Smart scrolling finished after {round_num + 1} rounds")
    
    async def _extract_text_by_selectors(self, page: Page, selectors: List[str]) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å–ø–∏—Å–∫—É —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤."""
        for selector in selectors:
            try:
                element = await page.query_selector(selector)
                if element:
                    text = await element.inner_text()
                    if text and text.strip():
                        return text.strip()
            except:
                continue
        return None
    
    def _extract_price(self, price_text: str) -> Optional[Decimal]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        if not price_text:
            return None
        
        # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä, —Ç–æ—á–µ–∫ –∏ –∑–∞–ø—è—Ç—ã—Ö
        clean_price = re.sub(r'[^\d.,]', '', price_text)
        
        # –ò—â–µ–º —á–∏—Å–ª–æ
        price_match = re.search(r'(\d+(?:[.,]\d+)?)', clean_price)
        if price_match:
            price_str = price_match.group(1).replace(',', '.')
            try:
                return Decimal(price_str)
            except:
                pass
        
        return None
    
    def _extract_weight_from_text(self, text: str) -> Optional[Decimal]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–µ—Å–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        if not text:
            return None
        
        # –ò—â–µ–º –≤–µ—Å –≤ –≥—Ä–∞–º–º–∞—Ö
        weight_patterns = [
            r'(\d+(?:[.,]\d+)?)\s*–≥\b',
            r'(\d+(?:[.,]\d+)?)\s*–≥—Ä\b',
            r'(\d+(?:[.,]\d+)?)\s*gram\b',
            r'(\d+(?:[.,]\d+)?)\s*g\b'
        ]
        
        for pattern in weight_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    return Decimal(match.group(1).replace(',', '.'))
                except:
                    continue
        
        return None
    
    def _extract_tags_from_text(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        if not text:
            return []
        
        tags = []
        text_lower = text.lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ç–µ–≥–æ–≤
        tag_keywords = {
            '–æ—Å—Ç—Ä–æ–µ': ['–æ—Å—Ç—Ä—ã–π', '–æ—Å—Ç—Ä–∞—è', '–æ—Å—Ç—Ä–æ–µ', '–ø–µ—Ä–µ—Ü', '—á–∏–ª–∏'],
            '–≤–µ–≥–µ—Ç–∞—Ä–∏–∞–Ω—Å–∫–æ–µ': ['–≤–µ–≥–µ—Ç–∞—Ä–∏–∞–Ω—Å–∫–∏–π', '–≤–µ–≥–µ—Ç–∞—Ä–∏–∞–Ω—Å–∫–∞—è', '–≤–µ–≥–∞–Ω'],
            '–¥–∏–µ—Ç–∏—á–µ—Å–∫–æ–µ': ['–¥–∏–µ—Ç–∏—á–µ—Å–∫–∏–π', '–¥–∏–µ—Ç–∏—á–µ—Å–∫–∞—è', '–ø–ø', '—Ñ–∏—Ç–Ω–µ—Å'],
            '–±–µ–∑ –≥–ª—é—Ç–µ–Ω–∞': ['–±–µ–∑ –≥–ª—é—Ç–µ–Ω–∞', '–±–µ–∑–≥–ª—é—Ç–µ–Ω–æ–≤—ã–π'],
            '–±–µ–∑ –ª–∞–∫—Ç–æ–∑—ã': ['–±–µ–∑ –ª–∞–∫—Ç–æ–∑—ã', '–±–µ–∑–ª–∞–∫—Ç–æ–∑–Ω—ã–π'],
            '–æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–æ–µ': ['–æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–π', '—ç–∫–æ', '–±–∏–æ'],
            '–¥–æ–º–∞—à–Ω–µ–µ': ['–¥–æ–º–∞—à–Ω–∏–π', '–¥–æ–º–∞—à–Ω—è—è', '—Ñ–µ—Ä–º–µ—Ä—Å–∫–∏–π']
        }
        
        for tag, keywords in tag_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                tags.append(tag)
        
        return tags
    
    async def _extract_text_from_element(self, element, selectors: List[str]) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º."""
        for selector in selectors:
            try:
                sub_element = await element.query_selector(selector)
                if sub_element:
                    text = await sub_element.inner_text()
                    if text and text.strip():
                        return text.strip()
            except:
                continue
        return None
    
    def _extract_id_from_url(self, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID —Ç–æ–≤–∞—Ä–∞ –∏–∑ URL."""
        # –ò—â–µ–º ID –≤ URL
        id_match = re.search(r'/product/([^/]+)', url)
        if id_match:
            return id_match.group(1)
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –ø—É—Ç–∏
        parsed = urlparse(url)
        path_parts = [p for p in parsed.path.split('/') if p]
        if path_parts:
            return path_parts[-1]
        
        # –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ö–µ—à –æ—Ç URL
        return super()._generate_id_from_url(url)
