#!/usr/bin/env python3
"""
üèóÔ∏è –¢–Ø–ñ–ï–õ–´–ô –ü–ê–†–°–ï–† –í–ö–£–°–í–ò–õ–õ–ê –î–õ–Ø –ú–û–°–ö–í–´ –° –ê–î–†–ï–°–û–ú
–ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å —É—Å—Ç–∞–Ω–æ–≤–∫–æ–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–¥—Ä–µ—Å–∞ –≤ –ú–æ—Å–∫–≤–µ.
"""

import asyncio
import csv
import json
import logging
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from urllib.parse import urljoin, quote

try:
    from selectolax.parser import HTMLParser
except ImportError:
    HTMLParser = None

# –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π AntiBotClient
import httpx


class AntiBotClient:
    """–ü—Ä–æ—Å—Ç–æ–π HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã."""
    
    def __init__(self, concurrency: int = 10, timeout: int = 30):
        self.semaphore = asyncio.Semaphore(concurrency)
        self.timeout = timeout
        
    async def request(self, method: str, url: str, **kwargs):
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å HTTP –∑–∞–ø—Ä–æ—Å."""
        async with self.semaphore:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            async with httpx.AsyncClient(timeout=self.timeout, headers=headers) as client:
                response = await client.request(method, url, **kwargs)
                return response
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –∫–ª–∏–µ–Ω—Ç–∞."""
        pass


class VkusvillHeavyParser:
    """–¢—è–∂–µ–ª—ã–π –ø–∞—Ä—Å–µ—Ä —Å –≥–ª—É–±–æ–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏."""
    
    def __init__(self, antibot_client):
        self.antibot_client = antibot_client
        self.BASE_URL = "https://vkusvill.ru"
        
    async def scrape_heavy_with_address(self, limit: int = 1500, address: str = "–¢–≤–µ—Ä—Å–∫–∞—è —É–ª–∏—Ü–∞, 12") -> List[Dict]:
        """–¢—è–∂–µ–ª—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å —É—Å—Ç–∞–Ω–æ–≤–∫–æ–π –∞–¥—Ä–µ—Å–∞."""
        print(f"üèóÔ∏è –ù–∞—á–∏–Ω–∞–µ–º —Ç—è–∂–µ–ª—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–∞ {limit} —Ç–æ–≤–∞—Ä–æ–≤...")
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ª–æ–∫–∞—Ü–∏–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∞–¥—Ä–µ—Å–æ–º
        await self._set_location_with_address(address)
        
        # –°–±–æ—Ä –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã
        print("üìã –°–æ–±–∏—Ä–∞–µ–º –í–°–ï —Ç–æ–≤–∞—Ä—ã –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã...")
        product_urls = set()
        
        ready_food_categories = [
            "/goods/gotovaya-eda/",
            "/goods/gotovaya-eda/novinki/",
            "/goods/gotovaya-eda/vtorye-blyuda/",
            "/goods/gotovaya-eda/vtorye-blyuda/vtorye-blyuda-s-myasom/",
            "/goods/gotovaya-eda/vtorye-blyuda/vtorye-blyuda-s-ptitsey/",
            "/goods/gotovaya-eda/vtorye-blyuda/vtorye-blyuda-s-ryboy-i-moreproduktami/",
            "/goods/gotovaya-eda/vtorye-blyuda/garniry-i-vtorye-blyuda-bez-myasa/",
            "/goods/gotovaya-eda/vtorye-blyuda/pasta-pitstsa/",
            "/goods/gotovaya-eda/salaty/",
            "/goods/gotovaya-eda/sendvichi-shaurma-i-burgery/",
            "/goods/gotovaya-eda/bolshe-belka-menshe-kaloriy/",
            "/goods/gotovaya-eda/bolshe-belka-menshe-kaloriy/malo-kaloriy/",
            "/goods/gotovaya-eda/bolshe-belka-menshe-kaloriy/bolshe-belka/",
            "/goods/gotovaya-eda/okroshki-i-letnie-supy/",
            "/goods/gotovaya-eda/supy/",
            "/goods/gotovaya-eda/zavtraki/",
            "/goods/gotovaya-eda/zavtraki/bliny-i-oladi/",
            "/goods/gotovaya-eda/zavtraki/syrniki-zapekanki-i-rikotniki/",
            "/goods/gotovaya-eda/zavtraki/omlety-i-zavtraki-s-yaytsom/",
            "/goods/gotovaya-eda/zavtraki/kashi/",
            "/goods/gotovaya-eda/zakuski/",
            "/goods/gotovaya-eda/rolly-i-sety/",
            "/goods/gotovaya-eda/onigiri/",
            "/goods/gotovaya-eda/pirogi-pirozhki-i-lepyeshki/",
            "/goods/gotovaya-eda/privezem-goryachim/",
            "/goods/gotovaya-eda/privezem-goryachim/goryachie-napitki/",
            "/goods/gotovaya-eda/tarelka-zdorovogo-pitaniya/",
            "/goods/gotovaya-eda/veganskie-i-postnye-blyuda/",
            "/goods/gotovaya-eda/semeynyy-format/",
            "/goods/gotovaya-eda/kombo-na-kazhdyy-den/",
            "/goods/gotovaya-eda/kukhni-mira/",
            "/goods/gotovaya-eda/kukhni-mira/aziatskaya-kukhnya/",
            "/goods/gotovaya-eda/kukhni-mira/russkaya-kukhnya/",
            "/goods/gotovaya-eda/kukhni-mira/kukhnya-kavkaza/",
            "/goods/gotovaya-eda/kukhni-mira/sredizemnomorskaya-kukhnya/",
            "/goods/gotovaya-eda/bliny-i-oladi/",
            "/goods/gotovaya-eda/khalyal/"
        ]
        
        for category in ready_food_categories:
            try:
                urls = await self._get_category_products_extended(category, 200)  # –ë–æ–ª—å—à–µ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                product_urls.update(urls)
                print(f"   {category}: +{len(urls)} —Ç–æ–≤–∞—Ä–æ–≤")
            except Exception as e:
                print(f"   ‚ùå {category}: {e}")
        
        print(f"üì¶ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(product_urls)} —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–æ–≤–∞—Ä—ã")
        
        # –ë–µ—Ä–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã
        product_list = list(product_urls)
        products = []
        
        # –ü–∞—Ä—Å–∏–º —Ç–æ–≤–∞—Ä—ã –±–∞—Ç—á–∞–º–∏
        batch_size = 8
        semaphore = asyncio.Semaphore(6)
        
        for i in range(0, len(product_list), batch_size):
            batch = product_list[i:i + batch_size]
            print(f"üîç –¢–æ–≤–∞—Ä—ã {i+1}-{min(i+batch_size, len(product_list))}/{len(product_list)}")
            
            async def process_product(url):
                async with semaphore:
                    return await self._extract_full_product(url)
            
            tasks = [process_product(url) for url in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, dict) and result:
                    if self._is_ready_food(result):
                        products.append(result)
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
                        if len(products) >= limit:
                            print(f"üéØ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç {limit} —Ç–æ–≤–∞—Ä–æ–≤")
                            return products
            
            await asyncio.sleep(1)
        
        print(f"üèÅ –¢—è–∂–µ–ª—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(products)} —Ç–æ–≤–∞—Ä–æ–≤ —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
        return products
    
    async def _set_location_with_address(self, address: str):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ª–æ–∫–∞—Ü–∏–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∞–¥—Ä–µ—Å–æ–º."""
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –≥–µ–æ–∫–æ–¥–∏–Ω–≥ –∞–¥—Ä–µ—Å–∞
            geocode_url = f"https://geocode-maps.yandex.ru/1.x/?format=json&geocode={quote(address + ', –ú–æ—Å–∫–≤–∞')}"
            response = await self.antibot_client.request(method="GET", url=geocode_url)
            
            lat, lon = 55.7558, 37.6176  # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ü–µ–Ω—Ç—Ä–∞ –ú–æ—Å–∫–≤—ã
            
            if response.status_code == 200:
                try:
                    geo_data = response.json()
                    pos = geo_data['response']['GeoObjectCollection']['featureMember'][0]['GeoObject']['Point']['pos']
                    lon, lat = map(float, pos.split())
                    print(f"üìç –ù–∞–π–¥–µ–Ω—ã –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è '{address}': {lat}, {lon}")
                except:
                    print(f"üìç –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–Ω—Ç—Ä –ú–æ—Å–∫–≤—ã: {lat}, {lon}")
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ª–æ–∫–∞—Ü–∏—é –≤ –í–∫—É—Å–í–∏–ª–ª
            location_url = f"{self.BASE_URL}/api/location?city=–ú–æ—Å–∫–≤–∞&lat={lat}&lon={lon}&address={quote(address)}"
            await self.antibot_client.request(method="GET", url=location_url)
            print(f"üìç –õ–æ–∫–∞—Ü–∏—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: {address} ({lat}, {lon})")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –ª–æ–∫–∞—Ü–∏–∏: {e}")
            # Fallback –Ω–∞ —Ü–µ–Ω—Ç—Ä –ú–æ—Å–∫–≤—ã
            location_url = f"{self.BASE_URL}/api/location?city=–ú–æ—Å–∫–≤–∞&lat=55.7558&lon=37.6176"
            await self.antibot_client.request(method="GET", url=location_url)
            print("üìç –õ–æ–∫–∞—Ü–∏—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: –ú–æ—Å–∫–≤–∞ (—Ü–µ–Ω—Ç—Ä)")
    
    async def _get_category_products_extended(self, category: str, max_products: int) -> List[str]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏."""
        product_urls = set()
        
        # –û–±—ã—á–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –ª–∏–º–∏—Ç–æ–º
        for page_num in range(1, 200):  # –î–æ 200 —Å—Ç—Ä–∞–Ω–∏—Ü
            try:
                url = f"{self.BASE_URL}{category}?page={page_num}"
                response = await self.antibot_client.request(method="GET", url=url)
                
                if response.status_code != 200:
                    break
                    
                parser = HTMLParser(response.text)
                links = parser.css('a[href*="/goods/"][href$=".html"]')
                
                if not links:
                    break

                page_count = 0
                for link in links:
                    href = link.attributes.get('href')
                    if href and '.html' in href and '/goods/' in href:
                        full_url = urljoin(self.BASE_URL, href)
                        if full_url not in product_urls:
                            product_urls.add(full_url)
                            page_count += 1

                if page_count == 0:
                    break
                    
                if len(product_urls) >= max_products:
                    break
                    
                await asyncio.sleep(0.1)  # –ë—ã—Å—Ç—Ä–µ–µ
                
            except Exception:
                break
        
        return list(product_urls)
    
    def _is_ready_food(self, product: Dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ —Ç–æ–≤–∞—Ä –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã."""
        name = product.get('name', '').lower()
        url = product.get('url', '').lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã
        ready_food_keywords = [
            '—Å—É–ø', '—Å–∞–ª–∞—Ç', '–±–æ—Ä—â', '–æ–º–ª–µ—Ç', '–±–ª–∏–Ω—ã', '–∫–∞—à–∞', '–ø–∏—Ü—Ü–∞',
            '–ø–∞—Å—Ç–∞', '–∫–æ—Ç–ª–µ—Ç–∞', '–∑–∞–ø–µ–∫–∞–Ω–∫–∞', '—Å—ã—Ä–Ω–∏–∫–∏', '–ø–ª–æ–≤', '–ª–∞–∑–∞–Ω—å—è',
            '–∫—Ä–µ–º-—Å—É–ø', '—Ö–∞—Ä—á–æ', '—Ü–µ–∑–∞—Ä—å', '–≤–∏–Ω–µ–≥—Ä–µ—Ç', '–º–∏–º–æ–∑–∞',
            '—Ä–∞–≥—É', '–≥—É–ª—è—à', '–∂–∞—Ä–∫–æ–µ', '–±–∏—Ç–æ—á–∫–∏', '—Ç–µ—Ñ—Ç–µ–ª–∏', '—Ñ—Ä–∏–∫–∞–¥–µ–ª—å–∫–∏',
            '–≥–æ–ª—É–±—Ü—ã', '–¥–æ–ª–º–∞', '–º–∞–Ω—Ç—ã', '–ø–µ–ª—å–º–µ–Ω–∏', '–≤–∞—Ä–µ–Ω–∏–∫–∏', '—Ö–∏–Ω–∫–∞–ª–∏',
            '—à–∞—É—Ä–º–∞', '–±—É—Ä–≥–µ—Ä', '—Å—ç–Ω–¥–≤–∏—á', '—Ä—É–ª–µ—Ç', '–ø–∏—Ä–æ–≥', '–∫–∏—à', '—Ç–∞—Ä—Ç',
            '—Ä–∏–∑–æ—Ç—Ç–æ', '–ø–∞—ç–ª—å—è', '–∫–∞—Ä—Ä–∏', '—Ä–∞–º–µ–Ω', '—Ñ–æ', '—Ç–æ–º-—è–º', '–º–∏—Å–æ',
            '–æ–∫—Ä–æ—à–∫–∞', '—Å–æ–ª—è–Ω–∫–∞', '—â–∏', '—É—Ö–∞', '—Ä–∞—Å—Å–æ–ª—å–Ω–∏–∫', '–∫—É–ª–µ—à',
            '–∑–∞–≤—Ç—Ä–∞–∫', '–æ–±–µ–¥', '—É–∂–∏–Ω', '–æ–Ω–∏–≥–∏—Ä–∏', '—Ä–æ–ª–ª', '—Å—É—à–∏'
        ]
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –ù–ï –≥–æ—Ç–æ–≤—É—é –µ–¥—É
        exclude_keywords = [
            '–∫—Ä–µ–º –¥–ª—è', '–≥–µ–ª—å –¥–ª—è', '—Å—Ä–µ–¥—Å—Ç–≤–æ –¥–ª—è', '–ø—Ä–æ–∫–ª–∞–¥–∫–∏', '–ø–æ–¥–≥—É–∑–Ω–∏–∫–∏',
            '—à–∞–º–ø—É–Ω—å', '–±–∞–ª—å–∑–∞–º', '–º—ã–ª–æ', '–∑—É–±–Ω–∞—è', '–ø–∞—Å—Ç–∞ –∑—É–±–Ω–∞—è',
            '—á–∏–ø—Å—ã', '—Å—É—Ö–∞—Ä–∏–∫–∏', '–æ—Ä–µ—Ö–∏', '—Å–µ–º–µ—á–∫–∏', '–∫–æ–Ω—Ñ–µ—Ç—ã', '—à–æ–∫–æ–ª–∞–¥',
            '–º–æ–ª–æ–∫–æ', '–∫–µ—Ñ–∏—Ä', '–π–æ–≥—É—Ä—Ç', '—Ç–≤–æ—Ä–æ–≥', '—Å—ã—Ä', '–º–∞—Å–ª–æ', '—è–π—Ü–∞',
            '–º—è—Å–æ', '–∫—É—Ä–∏—Ü–∞', '–≥–æ–≤—è–¥–∏–Ω–∞', '—Å–≤–∏–Ω–∏–Ω–∞', '—Ä—ã–±–∞', '—Ñ–∏–ª–µ',
            '–æ–≤–æ—â–∏', '—Ñ—Ä—É–∫—Ç—ã', '–∫–∞—Ä—Ç–æ—Ñ–µ–ª—å', '–∫–∞–ø—É—Å—Ç–∞', '–º–æ—Ä–∫–æ–≤—å',
            '—Ö–ª–µ–±', '–±–∞—Ç–æ–Ω', '–±—É–ª–∫–∞', '–±–∞–≥–µ—Ç', '–ª–∞–≤–∞—à'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL –Ω–∞ –≥–æ—Ç–æ–≤—É—é –µ–¥—É
        if 'gotovaya-eda' in url:
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã
        if any(keyword in name for keyword in ready_food_keywords):
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –Ω–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
            if not any(exclude in name for exclude in exclude_keywords):
                return True
        
        return False
    
    async def _extract_full_product(self, url: str, retry_count: int = 0) -> Optional[Dict]:
        """–ü–æ–ª–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ —Å–æ –≤—Å–µ–º–∏ –¥–∞–Ω–Ω—ã–º–∏ —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º."""
        max_retries = 1  # –£–º–µ–Ω—å—à–∞–µ–º retry –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        
        try:
            response = await self.antibot_client.request(method="GET", url=url)
            if response.status_code != 200 or not HTMLParser:
                return None
                
            parser = HTMLParser(response.text)
            page_text = response.text
            
            # –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            product = {
                'id': self._extract_id(url),
                'name': self._extract_name(parser, page_text),
                'price': self._extract_price(parser, page_text),
                'category': '–ì–æ—Ç–æ–≤–∞—è –µ–¥–∞',
                'url': url,
                'shop': 'vkusvill_heavy',
                'photo': self._extract_photo(parser),
                'composition': self._extract_composition(parser, page_text),
                'tags': '',
                'portion_g': self._extract_portion_weight(parser, page_text)
            }
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ë–ñ–£
            nutrition = self._extract_bju_comprehensive(parser, page_text)
            product.update(nutrition)
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ–ª—è–º
            filled_bju = sum(1 for field in ['kcal_100g', 'protein_100g', 'fat_100g', 'carb_100g'] if product.get(field))
            has_composition = bool(product.get('composition'))
            
            # –ö—Ä–∞—Ç–∫–∏–µ –ª–æ–≥–∏
            print(f"      üì¶ {product['name'][:40]}... –ë–ñ–£:{filled_bju}/4 –°–æ—Å—Ç–∞–≤:{'‚úì' if has_composition else '‚úó'} –¶–µ–Ω–∞:{product['price'] or '?'}")
            
            # Retry –¥–ª—è —Å–æ—Å—Ç–∞–≤–∞
            if not has_composition and retry_count < max_retries:
                await asyncio.sleep(0.5)
                return await self._extract_full_product(url, retry_count + 1)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if not product['name']:
                return None
            
            return product
            
        except Exception as e:
            if retry_count < max_retries:
                await asyncio.sleep(1)
                return await self._extract_full_product(url, retry_count + 1)
            return None
    
    def _extract_id(self, url: str) -> str:
        """ID —Ç–æ–≤–∞—Ä–∞ –∏–∑ URL."""
        match = re.search(r'/goods/([^/]+)\.html', url)
        return match.group(1) if match else str(hash(url))[-8:]
    
    def _extract_name(self, parser, page_text: str) -> str:
        """–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞."""
        selectors = ['h1', '.product-title', '.goods-title']
        for selector in selectors:
            element = parser.css_first(selector)
            if element and element.text(strip=True):
                return element.text(strip=True)[:150]
        return ""
    
    def _extract_price(self, parser, page_text: str) -> str:
        """–¶–µ–Ω–∞ —Ç–æ–≤–∞—Ä–∞."""
        selectors = ['.price', '.product-price', '.cost', '.goods-price']
        
        for selector in selectors:
            elements = parser.css(selector)
            for element in elements:
                price_text = element.text(strip=True)
                numbers = re.findall(r'(\d+(?:[.,]\d+)?)', price_text)
                for num in numbers:
                    try:
                        price_val = float(num.replace(',', '.'))
                        if 10 <= price_val <= 10000:
                            return num.replace(',', '.')
                    except ValueError:
                        continue
        
        return ""
    
    def _extract_photo(self, parser) -> str:
        """–§–æ—Ç–æ —Ç–æ–≤–∞—Ä–∞."""
        selectors = ['img[src*="product"]', 'img[src*="goods"]', 'img']
        
        for selector in selectors:
            elements = parser.css(selector)
            for element in elements:
                src = element.attributes.get('src') or element.attributes.get('data-src')
                if src and any(keyword in src.lower() for keyword in ['product', 'goods', 'catalog']):
                    if not any(skip in src.lower() for skip in ['icon', 'logo', 'banner']):
                        full_url = urljoin(self.BASE_URL, src)
                        if full_url.startswith('http'):
                            return full_url
        
        return ""
    
    def _extract_composition(self, parser, page_text: str) -> str:
        """–°–æ—Å—Ç–∞–≤ —Ç–æ–≤–∞—Ä–∞."""
        elements = parser.css('div, p, span, td, li')
        for element in elements:
            text = element.text().strip()
            text_lower = text.lower()
            
            if '—Å–æ—Å—Ç–∞–≤' in text_lower and len(text) > 10:
                if not any(word in text_lower for word in ['–º–µ–Ω—é', '–∫–∞—Ç–∞–ª–æ–≥', '–∫–æ—Ä–∑–∏–Ω–∞']):
                    if text_lower.startswith('—Å–æ—Å—Ç–∞–≤'):
                        return text[:800]
                    elif len(text) < 800:
                        return text[:500]
        
        return ""
    
    def _extract_portion_weight(self, parser, page_text: str) -> str:
        """–í–µ—Å –ø–æ—Ä—Ü–∏–∏."""
        patterns = [r'(\d+(?:[.,]\d+)?)\s*(?:–≥|–≥—Ä|–≥—Ä–∞–º)']
        for pattern in patterns:
            matches = re.finditer(pattern, page_text, re.I)
            for match in matches:
                weight = float(match.group(1).replace(',', '.'))
                if 10 <= weight <= 2000:
                    return f"{weight}–≥"
        return ""
    
    def _extract_bju_comprehensive(self, parser, page_text: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ë–ñ–£."""
        nutrition = {'kcal_100g': '', 'protein_100g': '', 'fat_100g': '', 'carb_100g': ''}
        
        # –ü–æ–∏—Å–∫ –≤ —ç–ª–µ–º–µ–Ω—Ç–∞—Ö —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        elements = parser.css('div, span, p, td, th, li')
        for element in elements:
            text = element.text().lower()
            original_text = element.text()
            
            if any(word in text for word in ['–∫–∫–∞–ª', '–±–µ–ª–∫–∏', '–∂–∏—Ä—ã', '—É–≥–ª–µ–≤–æ–¥—ã']):
                if ('–∫–∫–∞–ª' in text or '–∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç—å' in text) and not nutrition['kcal_100g']:
                    kcal_patterns = [r'(\d+(?:[.,]\d+)?)\s*–∫–∫–∞–ª', r'–∫–∫–∞–ª[:\s]*(\d+(?:[.,]\d+)?)']
                    for pattern in kcal_patterns:
                        match = re.search(pattern, text)
                        if match:
                            try:
                                val = float(match.group(1).replace(',', '.'))
                                if 10 <= val <= 900:
                                    nutrition['kcal_100g'] = match.group(1).replace(',', '.')
                                    break
                            except ValueError:
                                continue
                
                if '–±–µ–ª–∫' in text and not nutrition['protein_100g']:
                    protein_patterns = [r'(\d+(?:[.,]\d+)?)\s+–±–µ–ª–∫–∏,\s*–≥', r'–±–µ–ª–∫[–∏–∞][:\s]*(\d+(?:[.,]\d+)?)']
                    for pattern in protein_patterns:
                        match = re.search(pattern, text)
                        if match:
                            try:
                                val = float(match.group(1).replace(',', '.'))
                                if 0 <= val <= 100:
                                    nutrition['protein_100g'] = match.group(1).replace(',', '.')
                                    break
                            except ValueError:
                                continue
                
                if '–∂–∏—Ä' in text and not nutrition['fat_100g']:
                    fat_patterns = [r'(\d+(?:[.,]\d+)?)\s+–∂–∏—Ä—ã,\s*–≥', r'–∂–∏—Ä[—ã–∞][:\s]*(\d+(?:[.,]\d+)?)']
                    for pattern in fat_patterns:
                        match = re.search(pattern, text)
                        if match:
                            try:
                                val = float(match.group(1).replace(',', '.'))
                                if 0 <= val <= 100:
                                    nutrition['fat_100g'] = match.group(1).replace(',', '.')
                                    break
                            except ValueError:
                                continue
                
                if '—É–≥–ª–µ–≤–æ–¥' in text and not nutrition['carb_100g']:
                    carb_patterns = [r'(\d+(?:[.,]\d+)?)\s+—É–≥–ª–µ–≤–æ–¥—ã,\s*–≥', r'—É–≥–ª–µ–≤–æ–¥[—ã–∞][:\s]*(\d+(?:[.,]\d+)?)']
                    for pattern in carb_patterns:
                        match = re.search(pattern, text)
                        if match:
                            try:
                                val = float(match.group(1).replace(',', '.'))
                                if 0 <= val <= 100:
                                    nutrition['carb_100g'] = match.group(1).replace(',', '.')
                                    break
                            except ValueError:
                                continue
        
        return nutrition


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç—è–∂–µ–ª–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ —Å –∞–¥—Ä–µ—Å–æ–º."""
    limit = int(sys.argv[1]) if len(sys.argv) > 1 else 1500
    address = sys.argv[2] if len(sys.argv) > 2 else "–¢–≤–µ—Ä—Å–∫–∞—è —É–ª–∏—Ü–∞, 12"
    
    print("üèóÔ∏è –¢–Ø–ñ–ï–õ–´–ô –ü–ê–†–°–ï–† –í–ö–£–°–í–ò–õ–õ–ê - –ú–û–°–ö–í–ê –° –ê–î–†–ï–°–û–ú")
    print("=" * 60)
    print(f"üéØ –¶–µ–ª—å: {limit} —Ç–æ–≤–∞—Ä–æ–≤ —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
    print(f"üìç –ê–¥—Ä–µ—Å: {address}")
    print("‚ö° –†–µ–∂–∏–º: –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏")
    print()
    
    logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')
    
    antibot_client = AntiBotClient(concurrency=8, timeout=60)
    
    try:
        parser = VkusvillHeavyParser(antibot_client)
        
        start_time = time.time()
        products = await parser.scrape_heavy_with_address(limit, address)
        end_time = time.time()
        
        if not products:
            print("‚ùå –¢—è–∂–µ–ª—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ë–ñ–£ –∏ —Å–æ—Å—Ç–∞–≤–∞
        bju_stats = {'full_bju': 0, 'good_bju': 0, 'some_bju': 0, 'no_bju': 0}
        composition_stats = {'has_composition': 0, 'no_composition': 0}
        quality_stats = {'excellent': 0, 'good': 0, 'poor': 0}
        
        for product in products:
            bju_fields = ['kcal_100g', 'protein_100g', 'fat_100g', 'carb_100g']
            filled = sum(1 for field in bju_fields if product.get(field))
            has_composition = bool(product.get('composition'))
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ë–ñ–£
            if filled == 4:
                bju_stats['full_bju'] += 1
            elif filled == 3:
                bju_stats['good_bju'] += 1
            elif filled >= 1:
                bju_stats['some_bju'] += 1
            else:
                bju_stats['no_bju'] += 1
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Å—Ç–∞–≤–∞
            if has_composition:
                composition_stats['has_composition'] += 1
            else:
                composition_stats['no_composition'] += 1
            
            # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            if filled >= 3 and has_composition:
                quality_stats['excellent'] += 1
            elif filled >= 2 or has_composition:
                quality_stats['good'] += 1
            else:
                quality_stats['poor'] += 1
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        timestamp = int(time.time())
        csv_file = f"data/moscow_address_{timestamp}.csv"
        jsonl_file = f"data/moscow_address_{timestamp}.jsonl"
        
        Path("data").mkdir(exist_ok=True)
        
        if products:
            fieldnames = list(products[0].keys())
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(products)
        
        with open(jsonl_file, 'w', encoding='utf-8') as f:
            for product in products:
                f.write(json.dumps(product, ensure_ascii=False) + '\n')
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∏—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        duration = end_time - start_time
        print()
        print("üèÅ –¢–Ø–ñ–ï–õ–´–ô –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù")
        print("=" * 60)
        print(f"üìä –û–ë–©–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"   ‚Ä¢ –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}")
        print(f"   ‚Ä¢ –°–∫–æ—Ä–æ—Å—Ç—å: {len(products)/(duration/60):.1f} —Ç–æ–≤–∞—Ä–æ–≤/–º–∏–Ω")
        print()
        print(f"üçΩÔ∏è –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ë–ñ–£:")
        print(f"   ‚Ä¢ –ü–æ–ª–Ω–æ–µ –ë–ñ–£ (4/4): {bju_stats['full_bju']} ({bju_stats['full_bju']/len(products)*100:.1f}%)")
        print(f"   ‚Ä¢ –•–æ—Ä–æ—à–µ–µ –ë–ñ–£ (3/4): {bju_stats['good_bju']} ({bju_stats['good_bju']/len(products)*100:.1f}%)")
        print(f"   ‚Ä¢ –ß–∞—Å—Ç–∏—á–Ω–æ–µ –ë–ñ–£ (1-2/4): {bju_stats['some_bju']} ({bju_stats['some_bju']/len(products)*100:.1f}%)")
        print(f"   ‚Ä¢ –ë–µ–∑ –ë–ñ–£ (0/4): {bju_stats['no_bju']} ({bju_stats['no_bju']/len(products)*100:.1f}%)")
        print()
        print(f"üìù –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–û–°–¢–ê–í–ê:")
        print(f"   ‚Ä¢ –ï—Å—Ç—å —Å–æ—Å—Ç–∞–≤: {composition_stats['has_composition']} ({composition_stats['has_composition']/len(products)*100:.1f}%)")
        print(f"   ‚Ä¢ –ù–µ—Ç —Å–æ—Å—Ç–∞–≤–∞: {composition_stats['no_composition']} ({composition_stats['no_composition']/len(products)*100:.1f}%)")
        print()
        print(f"‚≠ê –û–ë–©–ï–ï –ö–ê–ß–ï–°–¢–í–û –î–ê–ù–ù–´–•:")
        print(f"   ‚Ä¢ –û—Ç–ª–∏—á–Ω–æ–µ (–ë–ñ–£ 3+ + —Å–æ—Å—Ç–∞–≤): {quality_stats['excellent']} ({quality_stats['excellent']/len(products)*100:.1f}%)")
        print(f"   ‚Ä¢ –•–æ—Ä–æ—à–µ–µ (–ë–ñ–£ 2+ –ò–õ–ò —Å–æ—Å—Ç–∞–≤): {quality_stats['good']} ({quality_stats['good']/len(products)*100:.1f}%)")
        print(f"   ‚Ä¢ –ü–ª–æ—Ö–æ–µ (–ë–ñ–£ <2 –ò –Ω–µ—Ç —Å–æ—Å—Ç–∞–≤–∞): {quality_stats['poor']} ({quality_stats['poor']/len(products)*100:.1f}%)")
        print()
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration/60:.1f} –º–∏–Ω—É—Ç")
        print(f"üíæ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"   ‚Ä¢ CSV: {csv_file}")
        print(f"   ‚Ä¢ JSONL: {jsonl_file}")
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç—è–∂–µ–ª–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
    finally:
        await antibot_client.close()


if __name__ == "__main__":
    asyncio.run(main())
