#!/usr/bin/env python3
"""
üèóÔ∏è –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–ê–†–°–ï–† –í–ö–£–°–í–ò–õ–õ–ê –î–õ–Ø –ú–û–°–ö–í–´
–ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤.
"""

import asyncio
import csv
import json
import logging
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from urllib.parse import urljoin, quote

try:
    from selectolax.parser import HTMLParser
except ImportError:
    HTMLParser = None

import httpx


class AntiBotClient:
    """–ü—Ä–æ—Å—Ç–æ–π HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã."""
    
    def __init__(self, concurrency: int = 10, timeout: int = 30):
        self.semaphore = asyncio.Semaphore(concurrency)
        self.timeout = timeout
        
    async def request(self, method: str, url: str, **kwargs):
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å HTTP –∑–∞–ø—Ä–æ—Å."""
        async with self.semaphore:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            async with httpx.AsyncClient(timeout=self.timeout, headers=headers) as client:
                response = await client.request(method, url, **kwargs)
                return response
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –∫–ª–∏–µ–Ω—Ç–∞."""
        pass


class VkusvillImprovedParser:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π."""
    
    def __init__(self, antibot_client):
        self.antibot_client = antibot_client
        self.BASE_URL = "https://vkusvill.ru"
        
    async def scrape_all_products(self, limit: int = 1500) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π."""
        print(f"üèóÔ∏è –ù–∞—á–∏–Ω–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–∞ {limit} —Ç–æ–≤–∞—Ä–æ–≤...")
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ª–æ–∫–∞—Ü–∏–∏ –¥–ª—è –ú–æ—Å–∫–≤—ã
        await self._set_location()
        
        # –°–±–æ—Ä —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π
        print("üìã –°–æ–±–∏—Ä–∞–µ–º –í–°–ï —Ç–æ–≤–∞—Ä—ã –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π...")
        product_urls = set()
        
        ready_food_categories = [
            "/goods/gotovaya-eda/",
            "/goods/gotovaya-eda/novinki/",
            "/goods/gotovaya-eda/vtorye-blyuda/",
            "/goods/gotovaya-eda/salaty/",
            "/goods/gotovaya-eda/sendvichi-shaurma-i-burgery/",
            "/goods/gotovaya-eda/supy/",
            "/goods/gotovaya-eda/zavtraki/",
            "/goods/gotovaya-eda/zakuski/",
            "/goods/gotovaya-eda/rolly-i-sety/",
            "/goods/gotovaya-eda/onigiri/",
            "/goods/gotovaya-eda/pirogi-pirozhki-i-lepyeshki/"
        ]
        
        for category in ready_food_categories:
            try:
                urls = await self._get_all_category_products(category)
                product_urls.update(urls)
                print(f"   {category}: +{len(urls)} —Ç–æ–≤–∞—Ä–æ–≤")
            except Exception as e:
                print(f"   ‚ùå {category}: {e}")
        
        print(f"üì¶ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(product_urls)} —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–æ–≤–∞—Ä—ã")
        
        # –ü–∞—Ä—Å–∏–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã
        products = []
        product_list = list(product_urls)
        
        batch_size = 10
        semaphore = asyncio.Semaphore(8)
        
        for i in range(0, len(product_list), batch_size):
            batch = product_list[i:i + batch_size]
            print(f"üîç –¢–æ–≤–∞—Ä—ã {i+1}-{min(i+batch_size, len(product_list))}/{len(product_list)}")
            
            async def process_product(url):
                async with semaphore:
                    return await self._extract_full_product(url)
            
            tasks = [process_product(url) for url in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, dict) and result:
                    if self._is_ready_food(result):
                        products.append(result)
                        
                        if len(products) >= limit:
                            print(f"üéØ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç {limit} —Ç–æ–≤–∞—Ä–æ–≤")
                            return products
            
            await asyncio.sleep(0.5)
        
        print(f"üèÅ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(products)} —Ç–æ–≤–∞—Ä–æ–≤")
        return products
    
    async def _set_location(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ª–æ–∫–∞—Ü–∏–∏ –¥–ª—è –ú–æ—Å–∫–≤—ã."""
        try:
            location_url = f"{self.BASE_URL}/api/location?city=–ú–æ—Å–∫–≤–∞&lat=55.7558&lon=37.6176"
            await self.antibot_client.request(method="GET", url=location_url)
            print("üìç –õ–æ–∫–∞—Ü–∏—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: –ú–æ—Å–∫–≤–∞ (—Ü–µ–Ω—Ç—Ä)")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –ª–æ–∫–∞—Ü–∏–∏: {e}")
    
    async def _get_all_category_products(self, category: str) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –í–°–ï —Ç–æ–≤–∞—Ä—ã –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —á–µ—Ä–µ–∑ –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã."""
        product_urls = set()
        
        # –ú–µ—Ç–æ–¥ 1: –û–±—ã—á–Ω–∞—è –ø–∞–≥–∏–Ω–∞—Ü–∏—è —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –ª–∏–º–∏—Ç–æ–º —Å—Ç—Ä–∞–Ω–∏—Ü
        print(f"      üîç –ü–∞–≥–∏–Ω–∞—Ü–∏—è –¥–ª—è {category}")
        page = 1
        consecutive_empty = 0
        
        while page <= 100 and consecutive_empty < 5:  # –î–æ 100 —Å—Ç—Ä–∞–Ω–∏—Ü, –Ω–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –ø–æ—Å–ª–µ 5 –ø—É—Å—Ç—ã—Ö –ø–æ–¥—Ä—è–¥
            try:
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã URL –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
                urls_to_try = [
                    f"{self.BASE_URL}{category}?page={page}",
                    f"{self.BASE_URL}{category}?PAGEN_1={page}",
                    f"{self.BASE_URL}{category}?p={page}",
                    f"{self.BASE_URL}{category}?offset={24*(page-1)}&limit=24",
                    f"{self.BASE_URL}{category}?start={24*(page-1)}&count=24"
                ]
                
                found_products = False
                
                for url in urls_to_try:
                    try:
                        response = await self.antibot_client.request(method="GET", url=url)
                        
                        if response.status_code != 200:
                            continue
                            
                        parser = HTMLParser(response.text)
                        links = parser.css('a[href*="/goods/"][href$=".html"]')
                        
                        if links:
                            page_count = 0
                            for link in links:
                                href = link.attributes.get('href')
                                if href and '.html' in href and '/goods/' in href:
                                    full_url = urljoin(self.BASE_URL, href)
                                    if full_url not in product_urls:
                                        product_urls.add(full_url)
                                        page_count += 1
                            
                            if page_count > 0:
                                print(f"         ‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: +{page_count} —Ç–æ–≤–∞—Ä–æ–≤ (–≤—Å–µ–≥–æ: {len(product_urls)})")
                                found_products = True
                                consecutive_empty = 0
                                break
                    
                    except Exception:
                        continue
                
                if not found_products:
                    consecutive_empty += 1
                    print(f"         ‚ùå –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –ø—É—Å—Ç–∞—è ({consecutive_empty}/5)")
                
                page += 1
                await asyncio.sleep(0.1)
                
            except Exception:
                consecutive_empty += 1
                page += 1
        
        # –ú–µ—Ç–æ–¥ 2: –ü–æ–ø—ã—Ç–∫–∞ AJAX –∑–∞–≥—Ä—É–∑–∫–∏
        if len(product_urls) < 200:  # –ï—Å–ª–∏ –º–∞–ª–æ —Ç–æ–≤–∞—Ä–æ–≤, –ø—Ä–æ–±—É–µ–º AJAX
            print(f"      üîÑ AJAX –∑–∞–≥—Ä—É–∑–∫–∞ –¥–ª—è {category}")
            await self._try_ajax_load(category, product_urls)
        
        # –ú–µ—Ç–æ–¥ 3: –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
        if len(product_urls) < 200:
            print(f"      üîÑ –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –¥–ª—è {category}")
            sort_params = ['price', 'name', 'popular', 'new', 'rating']
            for sort_param in sort_params:
                try:
                    url = f"{self.BASE_URL}{category}?sort={sort_param}"
                    response = await self.antibot_client.request(method="GET", url=url)
                    
                    if response.status_code == 200:
                        parser = HTMLParser(response.text)
                        links = parser.css('a[href*="/goods/"][href$=".html"]')
                        
                        sort_count = 0
                        for link in links:
                            href = link.attributes.get('href')
                            if href and '.html' in href and '/goods/' in href:
                                full_url = urljoin(self.BASE_URL, href)
                                if full_url not in product_urls:
                                    product_urls.add(full_url)
                                    sort_count += 1
                        
                        if sort_count > 0:
                            print(f"         ‚úÖ –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ {sort_param}: +{sort_count} —Ç–æ–≤–∞—Ä–æ–≤")
                        
                        await asyncio.sleep(0.2)
                        
                except Exception:
                    continue
        
        return list(product_urls)
    
    async def _try_ajax_load(self, category: str, product_urls: set):
        """–ü–æ–ø—ã—Ç–∫–∞ AJAX –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤."""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            base_url = f"{self.BASE_URL}{category}"
            response = await self.antibot_client.request(method="GET", url=base_url)
            
            if response.status_code != 200:
                return
            
            # –ò—â–µ–º AJAX endpoints –≤ JavaScript –∫–æ–¥–µ
            ajax_patterns = [
                r'url\s*:\s*["\']([^"\']*load[^"\']*)["\']',
                r'["\']([^"\']*ajax[^"\']*load[^"\']*)["\']',
                r'["\']([^"\']*catalog[^"\']*load[^"\']*)["\']'
            ]
            
            ajax_urls = set()
            for pattern in ajax_patterns:
                matches = re.findall(pattern, response.text, re.I)
                for match in matches:
                    if match.startswith('/'):
                        ajax_urls.add(self.BASE_URL + match)
                    elif match.startswith('http'):
                        ajax_urls.add(match)
            
            # –ü—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ AJAX endpoints
            standard_endpoints = [
                f"{self.BASE_URL}/ajax/catalog/",
                f"{self.BASE_URL}/ajax/goods/",
                f"{self.BASE_URL}/local/ajax/catalog.php",
                f"{self.BASE_URL}/bitrix/components/custom/catalog.section/ajax.php"
            ]
            ajax_urls.update(standard_endpoints)
            
            # –ü—Ä–æ–±—É–µ–º –∫–∞–∂–¥—ã–π endpoint
            for ajax_url in list(ajax_urls)[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
                try:
                    # POST –∑–∞–ø—Ä–æ—Å —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                    data = {
                        'PAGEN_1': 2,
                        'SECTION_CODE': category.strip('/').split('/')[-1],
                        'action': 'load_more',
                        'page': 2
                    }
                    
                    response = await self.antibot_client.request(
                        method="POST",
                        url=ajax_url,
                        data=data,
                        headers={
                            'Content-Type': 'application/x-www-form-urlencoded',
                            'X-Requested-With': 'XMLHttpRequest',
                            'Referer': base_url
                        }
                    )
                    
                    if response.status_code == 200 and len(response.text) > 100:
                        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ç–æ–≤–∞—Ä—ã –≤ –æ—Ç–≤–µ—Ç–µ
                        if 'goods/' in response.text and '.html' in response.text:
                            parser = HTMLParser(response.text)
                            links = parser.css('a[href*="/goods/"][href$=".html"]')
                            
                            ajax_count = 0
                            for link in links:
                                href = link.attributes.get('href')
                                if href and '.html' in href and '/goods/' in href:
                                    full_url = urljoin(self.BASE_URL, href)
                                    if full_url not in product_urls:
                                        product_urls.add(full_url)
                                        ajax_count += 1
                            
                            if ajax_count > 0:
                                print(f"         ‚úÖ AJAX {ajax_url}: +{ajax_count} —Ç–æ–≤–∞—Ä–æ–≤")
                                return  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ä–∞–±–æ—á–∏–π endpoint, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
                
                except Exception:
                    continue
                    
        except Exception:
            pass
    
    def _is_ready_food(self, product: Dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ —Ç–æ–≤–∞—Ä –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã."""
        name = product.get('name', '').lower()
        url = product.get('url', '').lower()
        
        # –ï—Å–ª–∏ URL —Å–æ–¥–µ—Ä–∂–∏—Ç –≥–æ—Ç–æ–≤—É—é –µ–¥—É - —Ç–æ—á–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç
        if 'gotovaya-eda' in url:
            return True
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã
        ready_food_keywords = [
            '—Å—É–ø', '—Å–∞–ª–∞—Ç', '–±–æ—Ä—â', '–æ–º–ª–µ—Ç', '–±–ª–∏–Ω—ã', '–∫–∞—à–∞', '–ø–∏—Ü—Ü–∞',
            '–ø–∞—Å—Ç–∞', '–∫–æ—Ç–ª–µ—Ç–∞', '–∑–∞–ø–µ–∫–∞–Ω–∫–∞', '—Å—ã—Ä–Ω–∏–∫–∏', '–ø–ª–æ–≤', '–ª–∞–∑–∞–Ω—å—è',
            '—à–∞—É—Ä–º–∞', '–±—É—Ä–≥–µ—Ä', '—Å—ç–Ω–¥–≤–∏—á', '–æ–Ω–∏–≥–∏—Ä–∏', '—Ä–æ–ª–ª', '—Å—É—à–∏'
        ]
        
        return any(keyword in name for keyword in ready_food_keywords)
    
    async def _extract_full_product(self, url: str) -> Optional[Dict]:
        """–ü–æ–ª–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞."""
        try:
            response = await self.antibot_client.request(method="GET", url=url)
            if response.status_code != 200 or not HTMLParser:
                return None
                
            parser = HTMLParser(response.text)
            page_text = response.text
            
            # –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            product = {
                'id': self._extract_id(url),
                'name': self._extract_name(parser, page_text),
                'price': self._extract_price(parser, page_text),
                'category': '–ì–æ—Ç–æ–≤–∞—è –µ–¥–∞',
                'url': url,
                'shop': 'vkusvill_improved',
                'photo': self._extract_photo(parser),
                'composition': self._extract_composition(parser, page_text),
                'tags': '',
                'portion_g': self._extract_portion_weight(parser, page_text)
            }
            
            # –ë–ñ–£
            nutrition = self._extract_bju(parser, page_text)
            product.update(nutrition)
            
            # –ö—Ä–∞—Ç–∫–∏–µ –ª–æ–≥–∏
            filled_bju = sum(1 for field in ['kcal_100g', 'protein_100g', 'fat_100g', 'carb_100g'] if product.get(field))
            has_composition = bool(product.get('composition'))
            
            print(f"      üì¶ {product['name'][:40]}... –ë–ñ–£:{filled_bju}/4 –°–æ—Å—Ç–∞–≤:{'‚úì' if has_composition else '‚úó'} –¶–µ–Ω–∞:{product['price'] or '?'}")
            
            if not product['name']:
                return None
            
            return product
            
        except Exception:
            return None
    
    def _extract_id(self, url: str) -> str:
        """ID —Ç–æ–≤–∞—Ä–∞ –∏–∑ URL."""
        match = re.search(r'/goods/([^/]+)\.html', url)
        return match.group(1) if match else str(hash(url))[-8:]
    
    def _extract_name(self, parser, page_text: str) -> str:
        """–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞."""
        selectors = ['h1', '.product-title', '.goods-title']
        for selector in selectors:
            element = parser.css_first(selector)
            if element and element.text(strip=True):
                return element.text(strip=True)[:150]
        return ""
    
    def _extract_price(self, parser, page_text: str) -> str:
        """–¶–µ–Ω–∞ —Ç–æ–≤–∞—Ä–∞."""
        selectors = ['.price', '.product-price', '.cost', '.goods-price']
        
        for selector in selectors:
            elements = parser.css(selector)
            for element in elements:
                price_text = element.text(strip=True)
                numbers = re.findall(r'(\d+(?:[.,]\d+)?)', price_text)
                for num in numbers:
                    try:
                        price_val = float(num.replace(',', '.'))
                        if 10 <= price_val <= 10000:
                            return num.replace(',', '.')
                    except ValueError:
                        continue
        
        return ""
    
    def _extract_photo(self, parser) -> str:
        """–§–æ—Ç–æ —Ç–æ–≤–∞—Ä–∞."""
        selectors = ['img[src*="product"]', 'img[src*="goods"]', 'img']
        
        for selector in selectors:
            elements = parser.css(selector)
            for element in elements:
                src = element.attributes.get('src') or element.attributes.get('data-src')
                if src and any(keyword in src.lower() for keyword in ['product', 'goods', 'catalog']):
                    if not any(skip in src.lower() for skip in ['icon', 'logo', 'banner']):
                        full_url = urljoin(self.BASE_URL, src)
                        if full_url.startswith('http'):
                            return full_url
        
        return ""
    
    def _extract_composition(self, parser, page_text: str) -> str:
        """–°–æ—Å—Ç–∞–≤ —Ç–æ–≤–∞—Ä–∞."""
        elements = parser.css('div, p, span, td, li')
        for element in elements:
            text = element.text().strip()
            text_lower = text.lower()
            
            if '—Å–æ—Å—Ç–∞–≤' in text_lower and len(text) > 10:
                if not any(word in text_lower for word in ['–º–µ–Ω—é', '–∫–∞—Ç–∞–ª–æ–≥', '–∫–æ—Ä–∑–∏–Ω–∞']):
                    if text_lower.startswith('—Å–æ—Å—Ç–∞–≤'):
                        return text[:800]
                    elif len(text) < 800:
                        return text[:500]
        
        return ""
    
    def _extract_portion_weight(self, parser, page_text: str) -> str:
        """–í–µ—Å –ø–æ—Ä—Ü–∏–∏."""
        patterns = [r'(\d+(?:[.,]\d+)?)\s*(?:–≥|–≥—Ä|–≥—Ä–∞–º)']
        for pattern in patterns:
            matches = re.finditer(pattern, page_text, re.I)
            for match in matches:
                try:
                    weight = float(match.group(1).replace(',', '.'))
                    if 10 <= weight <= 2000:
                        return f"{weight}–≥"
                except ValueError:
                    continue
        return ""
    
    def _extract_bju(self, parser, page_text: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ë–ñ–£."""
        nutrition = {'kcal_100g': '', 'protein_100g': '', 'fat_100g': '', 'carb_100g': ''}
        
        # –ü–æ–∏—Å–∫ –≤ —ç–ª–µ–º–µ–Ω—Ç–∞—Ö —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        elements = parser.css('div, span, p, td, th, li')
        for element in elements:
            text = element.text().lower()
            
            if any(word in text for word in ['–∫–∫–∞–ª', '–±–µ–ª–∫–∏', '–∂–∏—Ä—ã', '—É–≥–ª–µ–≤–æ–¥—ã']):
                if ('–∫–∫–∞–ª' in text or '–∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç—å' in text) and not nutrition['kcal_100g']:
                    match = re.search(r'(\d+(?:[.,]\d+)?)\s*–∫–∫–∞–ª', text)
                    if match:
                        try:
                            val = float(match.group(1).replace(',', '.'))
                            if 10 <= val <= 900:
                                nutrition['kcal_100g'] = match.group(1).replace(',', '.')
                        except ValueError:
                            pass
                
                if '–±–µ–ª–∫' in text and not nutrition['protein_100g']:
                    match = re.search(r'(\d+(?:[.,]\d+)?)\s+–±–µ–ª–∫–∏,\s*–≥', text)
                    if match:
                        try:
                            val = float(match.group(1).replace(',', '.'))
                            if 0 <= val <= 100:
                                nutrition['protein_100g'] = match.group(1).replace(',', '.')
                        except ValueError:
                            pass
                
                if '–∂–∏—Ä' in text and not nutrition['fat_100g']:
                    match = re.search(r'(\d+(?:[.,]\d+)?)\s+–∂–∏—Ä—ã,\s*–≥', text)
                    if match:
                        try:
                            val = float(match.group(1).replace(',', '.'))
                            if 0 <= val <= 100:
                                nutrition['fat_100g'] = match.group(1).replace(',', '.')
                        except ValueError:
                            pass
                
                if '—É–≥–ª–µ–≤–æ–¥' in text and not nutrition['carb_100g']:
                    match = re.search(r'(\d+(?:[.,]\d+)?)\s+—É–≥–ª–µ–≤–æ–¥—ã,\s*–≥', text)
                    if match:
                        try:
                            val = float(match.group(1).replace(',', '.'))
                            if 0 <= val <= 100:
                                nutrition['carb_100g'] = match.group(1).replace(',', '.')
                        except ValueError:
                            pass
        
        return nutrition


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞."""
    limit = int(sys.argv[1]) if len(sys.argv) > 1 else 1500
    
    print("üèóÔ∏è –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–ê–†–°–ï–† –í–ö–£–°–í–ò–õ–õ–ê - –ú–û–°–ö–í–ê")
    print("=" * 60)
    print(f"üéØ –¶–µ–ª—å: {limit} —Ç–æ–≤–∞—Ä–æ–≤ —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
    print("üìç –ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ: –ú–æ—Å–∫–≤–∞")
    print("‚ö° –†–µ–∂–∏–º: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –ø–∞–≥–∏–Ω–∞—Ü–∏—è")
    print()
    
    logging.basicConfig(level=logging.WARNING)
    
    antibot_client = AntiBotClient(concurrency=10, timeout=60)
    
    try:
        parser = VkusvillImprovedParser(antibot_client)
        
        start_time = time.time()
        products = await parser.scrape_all_products(limit)
        end_time = time.time()
        
        if not products:
            print("‚ùå –ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        bju_stats = {'full_bju': 0, 'good_bju': 0, 'some_bju': 0, 'no_bju': 0}
        composition_stats = {'has_composition': 0, 'no_composition': 0}
        
        for product in products:
            bju_fields = ['kcal_100g', 'protein_100g', 'fat_100g', 'carb_100g']
            filled = sum(1 for field in bju_fields if product.get(field))
            has_composition = bool(product.get('composition'))
            
            if filled == 4:
                bju_stats['full_bju'] += 1
            elif filled == 3:
                bju_stats['good_bju'] += 1
            elif filled >= 1:
                bju_stats['some_bju'] += 1
            else:
                bju_stats['no_bju'] += 1
            
            if has_composition:
                composition_stats['has_composition'] += 1
            else:
                composition_stats['no_composition'] += 1
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        timestamp = int(time.time())
        csv_file = f"data/moscow_improved_{timestamp}.csv"
        jsonl_file = f"data/moscow_improved_{timestamp}.jsonl"
        
        Path("data").mkdir(exist_ok=True)
        
        if products:
            fieldnames = list(products[0].keys())
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(products)
        
        with open(jsonl_file, 'w', encoding='utf-8') as f:
            for product in products:
                f.write(json.dumps(product, ensure_ascii=False) + '\n')
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        duration = end_time - start_time
        excellent_quality = bju_stats['full_bju'] + bju_stats['good_bju']
        
        print()
        print("üèÅ –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù")
        print("=" * 60)
        print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"   ‚Ä¢ –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}")
        print(f"   ‚Ä¢ –ü–æ–ª–Ω–æ–µ –ë–ñ–£ (4/4): {bju_stats['full_bju']} ({bju_stats['full_bju']/len(products)*100:.1f}%)")
        print(f"   ‚Ä¢ –•–æ—Ä–æ—à–µ–µ –ë–ñ–£ (3/4): {bju_stats['good_bju']} ({bju_stats['good_bju']/len(products)*100:.1f}%)")
        print(f"   ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {excellent_quality} ({excellent_quality/len(products)*100:.1f}%)")
        print(f"   ‚Ä¢ –ï—Å—Ç—å —Å–æ—Å—Ç–∞–≤: {composition_stats['has_composition']} ({composition_stats['has_composition']/len(products)*100:.1f}%)")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è: {duration/60:.1f} –º–∏–Ω—É—Ç")
        print(f"üíæ –§–∞–π–ª—ã: {csv_file}, {jsonl_file}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ —Ü–µ–ª–∏
        if len(products) >= 1200 and excellent_quality/len(products) >= 0.95:
            print("üéâ –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê: >1200 —Ç–æ–≤–∞—Ä–æ–≤ –∏ >95% –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö!")
        else:
            print(f"‚ö†Ô∏è  –¶–µ–ª—å –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞: –Ω—É–∂–Ω–æ >1200 —Ç–æ–≤–∞—Ä–æ–≤ (–µ—Å—Ç—å {len(products)}) –∏ >95% –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö (–µ—Å—Ç—å {excellent_quality/len(products)*100:.1f}%)")
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    finally:
        await antibot_client.close()


if __name__ == "__main__":
    asyncio.run(main())
