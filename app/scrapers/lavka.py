"""–°–∫—Ä–µ–π–ø–µ—Ä –¥–ª—è –Ø–Ω–¥–µ–∫—Å –õ–∞–≤–∫–∞."""

import asyncio
import re
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin, urlparse
from decimal import Decimal
from playwright.async_api import Page
from loguru import logger

from .base import BaseScraper


class LavkaScraper(BaseScraper):
    """–°–∫—Ä–µ–π–ø–µ—Ä –¥–ª—è —Å–∞–π—Ç–∞ –Ø–Ω–¥–µ–∫—Å –õ–∞–≤–∫–∞."""
    
    @property
    def shop_name(self) -> str:
        return "lavka"
    
    @property
    def base_url(self) -> str:
        return "https://lavka.yandex.ru"
    
    async def _set_location(self) -> None:
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≥–æ—Ä–æ–¥–∞ –∏ –∞–¥—Ä–µ—Å–∞ –≤ –Ø–Ω–¥–µ–∫—Å –õ–∞–≤–∫–∞."""
        page = await self.context.new_page()
        
        try:
            logger.info(f"Setting location to {self.config.city}, {self.config.address}")
            
            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            await page.goto(self.base_url, wait_until="networkidle")
            await asyncio.sleep(3)
            
            # –ò—â–µ–º –º–æ–¥–∞–ª—å–Ω–æ–µ –æ–∫–Ω–æ –≤—ã–±–æ—Ä–∞ –∞–¥—Ä–µ—Å–∞
            address_modal_selectors = [
                '[data-testid="address-modal"]',
                '.address-modal',
                '.location-modal',
                '[class*="modal"]',
                '[role="dialog"]'
            ]
            
            modal_found = False
            for selector in address_modal_selectors:
                try:
                    modal = await page.wait_for_selector(selector, timeout=5000)
                    if modal:
                        modal_found = True
                        break
                except:
                    continue
            
            if not modal_found:
                # –ò—â–µ–º –∫–Ω–æ–ø–∫—É –≤—ã–±–æ—Ä–∞ –∞–¥—Ä–µ—Å–∞
                address_button_selectors = [
                    '[data-testid="address-button"]',
                    '.address-button',
                    'button[class*="address"]',
                    'button:has-text("–í—ã–±–µ—Ä–∏—Ç–µ –∞–¥—Ä–µ—Å")',
                    'button:has-text("–ê–¥—Ä–µ—Å –¥–æ—Å—Ç–∞–≤–∫–∏")'
                ]
                
                for selector in address_button_selectors:
                    try:
                        button = await page.wait_for_selector(selector, timeout=3000)
                        if button:
                            await button.click()
                            await asyncio.sleep(2)
                            break
                    except:
                        continue
            
            # –ò—â–µ–º –ø–æ–ª–µ –≤–≤–æ–¥–∞ –∞–¥—Ä–µ—Å–∞
            address_input_selectors = [
                'input[placeholder*="–∞–¥—Ä–µ—Å"]',
                'input[placeholder*="–ê–¥—Ä–µ—Å"]',
                'input[name="address"]',
                'input[data-testid="address-input"]',
                '.address-input input',
                'input[type="text"]'
            ]
            
            address_input = None
            for selector in address_input_selectors:
                try:
                    address_input = await page.wait_for_selector(selector, timeout=5000)
                    if address_input:
                        break
                except:
                    continue
            
            if address_input:
                # –û—á–∏—â–∞–µ–º –ø–æ–ª–µ –∏ –≤–≤–æ–¥–∏–º –∞–¥—Ä–µ—Å
                await address_input.click()
                await address_input.fill("")
                full_address = f"{self.config.city}, {self.config.address}"
                await address_input.type(full_address, delay=100)
                await asyncio.sleep(3)
                
                # –ñ–¥–µ–º –ø–æ—è–≤–ª–µ–Ω–∏—è –ø–æ–¥—Å–∫–∞–∑–æ–∫ –∏ –∫–ª–∏–∫–∞–µ–º –Ω–∞ –ø–µ—Ä–≤—É—é
                suggestion_selectors = [
                    '[data-testid="address-suggestion"]',
                    '.address-suggestion',
                    '.suggestion',
                    '.dropdown-item',
                    '.autocomplete-item',
                    '[role="option"]'
                ]
                
                suggestion_clicked = False
                for selector in suggestion_selectors:
                    try:
                        suggestions = await page.wait_for_selector(selector, timeout=5000)
                        if suggestions:
                            await suggestions.click()
                            suggestion_clicked = True
                            break
                    except:
                        continue
                
                if not suggestion_clicked:
                    # –ï—Å–ª–∏ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –ø—Ä–æ—Å—Ç–æ –Ω–∞–∂–∏–º–∞–µ–º Enter
                    await address_input.press('Enter')
                
                await asyncio.sleep(2)
                
                # –ò—â–µ–º –∫–Ω–æ–ø–∫—É –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
                confirm_selectors = [
                    'button:has-text("–ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å")',
                    'button:has-text("–í—ã–±—Ä–∞—Ç—å")',
                    'button:has-text("–ì–æ—Ç–æ–≤–æ")',
                    'button:has-text("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å")',
                    '[data-testid="confirm-button"]',
                    '.confirm-button'
                ]
                
                for selector in confirm_selectors:
                    try:
                        confirm_btn = await page.wait_for_selector(selector, timeout=3000)
                        if confirm_btn:
                            await confirm_btn.click()
                            break
                    except:
                        continue
            
            await asyncio.sleep(5)
            logger.info("Location set successfully")
            
        except Exception as e:
            logger.warning(f"Could not set location automatically: {e}")
        finally:
            await page.close()
    
    async def _scrape_items(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã."""
        all_items = []
        
        # –ü—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã –Ø–Ω–¥–µ–∫—Å –õ–∞–≤–∫–∞
        category_urls = [
            "https://lavka.yandex.ru/category/gotovaya_eda",
            "https://lavka.yandex.ru/category/hot_streetfood", 
            "https://lavka.yandex.ru/category/gotovaya_eda/ostroe-1"
        ]
        
        for category_url in category_urls:
            try:
                category_name = category_url.split('/')[-1]
                logger.info(f"Scraping category URL: {category_url}")
                category_items = await self._scrape_category_url(category_url, category_name)
                all_items.extend(category_items)
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"Failed to scrape category {category_url}: {e}")
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
        seen_urls = set()
        unique_items = []
        for item in all_items:
            if item.get('url') not in seen_urls:
                seen_urls.add(item.get('url'))
                unique_items.append(item)
        
        logger.info(f"Found {len(unique_items)} unique items from {len(all_items)} total")
        return unique_items
    
    async def _scrape_category_url(self, category_url: str, category_name: str) -> List[Dict[str, Any]]:
        """–°–∫—Ä–µ–π–ø–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ –ø—Ä—è–º–æ–º—É URL –∫–∞—Ç–µ–≥–æ—Ä–∏–∏."""
        page = await self.context.new_page()
        items = []
        
        try:
            logger.info(f"üåê Loading category URL: {category_url}")
            
            await page.goto(category_url, wait_until="domcontentloaded", timeout=60000)
            logger.info(f"‚úÖ Page loaded successfully")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            title = await page.title()
            logger.info(f"üìÑ Page title: {title}")
            
            await asyncio.sleep(5)
            logger.info(f"‚è±Ô∏è Waited 5 seconds for page to settle")
            
            # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤
            await self._scroll_to_load_all(page)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–≤–∞—Ä—ã —Å —Ä–∞–±–æ—á–∏–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏ –¥–ª—è –õ–∞–≤–∫–∞
            product_selectors = [
                '[class*="Product"]',
                '[class*="Card"]',
                '[class*="product"]',
                'a[href*="/product/"]',
                'a[href*="/goods/"]',
                '[data-testid="product-card"]',
                '[data-testid="product"]',
                '.product-card',
                '.product',
                '.catalog-item'
            ]
            
            products = []
            logger.info(f"üîç Testing product selectors for Lavka...")
            
            for selector in product_selectors:
                try:
                    test_products = await page.query_selector_all(selector)
                    logger.info(f"   {selector}: {len(test_products)} elements")
                    if test_products and len(test_products) > len(products):
                        products = test_products
                        logger.info(f"‚úÖ Best selector so far: {selector} with {len(products)} products")
                except Exception as e:
                    logger.warning(f"   {selector}: ERROR - {e}")
            
            if not products:
                logger.warning(f"‚ùå No products found with standard selectors")
                
                # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                page_content = await page.content()
                logger.info(f"üìÑ Page content length: {len(page_content)} characters")
                
                if '—Ç–æ–≤–∞—Ä' in page_content.lower():
                    logger.info(f"‚úÖ Page contains word '—Ç–æ–≤–∞—Ä'")
                else:
                    logger.warning(f"‚ùå Page does not contain word '—Ç–æ–≤–∞—Ä'")
                
                if 'product' in page_content.lower():
                    logger.info(f"‚úÖ Page contains word 'product'")
                else:
                    logger.warning(f"‚ùå Page does not contain word 'product'")
                
                # Fallback: –∏—â–µ–º –ª—é–±—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã
                logger.info(f"üîç Looking for any product links...")
                link_selectors = ['a[href*="/product/"]', 'a[href*="/goods/"]', 'a[href*="/item/"]']
                
                for link_selector in link_selectors:
                    try:
                        links = await page.query_selector_all(link_selector)
                        logger.info(f"   {link_selector}: {len(links)} links")
                        if links:
                            products = links[:20]
                            break
                    except Exception as e:
                        logger.warning(f"   {link_selector}: ERROR - {e}")
                
                if not products:
                    logger.error(f"‚ùå No product links found at all!")
                    return items
            
            logger.info(f"Processing {len(products)} products from category {category_name}")
            
            for product in products:
                try:
                    item_data = await self._extract_product_data(product, page, category_name)
                    if item_data:
                        items.append(item_data)
                except Exception as e:
                    logger.warning(f"Failed to extract product data: {e}")
            
        except Exception as e:
            logger.error(f"Failed to scrape category {category_url}: {e}")
        finally:
            await page.close()
        
        return items
    
    async def _scrape_category(self, category: str) -> List[Dict[str, Any]]:
        """–°–∫—Ä–µ–π–ø–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏."""
        page = await self.context.new_page()
        items = []
        
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º URL –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            category_url = f"{self.base_url}/catalog/{category}"
            logger.debug(f"Loading category URL: {category_url}")
            
            await page.goto(category_url, wait_until="networkidle")
            await asyncio.sleep(3)
            
            # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤
            await self._scroll_to_load_all(page)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–≤–∞—Ä—ã
            product_selectors = [
                '[data-testid="product-card"]',
                '[data-testid="product"]',
                '.product-card',
                '.product',
                '[class*="ProductCard"]',
                '[class*="product"]'
            ]
            
            products = []
            for selector in product_selectors:
                try:
                    products = await page.query_selector_all(selector)
                    if products:
                        logger.debug(f"Found {len(products)} products with selector: {selector}")
                        break
                except:
                    continue
            
            if not products:
                logger.warning(f"No products found in category {category}")
                return items
            
            logger.info(f"Processing {len(products)} products from category {category}")
            
            for product in products:
                try:
                    item_data = await self._extract_product_data(product, page, category)
                    if item_data:
                        items.append(item_data)
                except Exception as e:
                    logger.warning(f"Failed to extract product data: {e}")
            
        except Exception as e:
            logger.error(f"Failed to scrape category {category}: {e}")
        finally:
            await page.close()
        
        return items
    
    async def _extract_product_data(self, product_element, page: Page, category: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–∞ –∏–∑ –∫–∞—Ä—Ç–æ—á–∫–∏."""
        try:
            # –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ - —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –õ–∞–≤–∫–∞
            name_selectors = [
                '[data-testid="product-name"]',
                '[data-testid="product-title"]',
                '.product-name',
                '.product-title',
                '[class*="title"]',
                '[class*="Title"]',
                '[class*="name"]',
                '[class*="Name"]',
                'h1', 'h2', 'h3', 'h4', 'h5',
                'a',
                'span'
            ]
            
            name = None
            for selector in name_selectors:
                try:
                    name_element = await product_element.query_selector(selector)
                    if name_element:
                        name = await name_element.inner_text()
                        if name and name.strip():
                            break
                except:
                    continue
            
            if not name:
                return None
            
            # –¶–µ–Ω–∞ - —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –õ–∞–≤–∫–∞
            price_selectors = [
                '[data-testid="product-price"]',
                '[data-testid="price"]',
                '.product-price',
                '.price',
                '[class*="Price"]',
                '[class*="price"]',
                '[class*="cost"]',
                '[class*="Cost"]',
                'span:contains("‚ÇΩ")',
                'div:contains("‚ÇΩ")'
            ]
            
            price = None
            for selector in price_selectors:
                try:
                    price_element = await product_element.query_selector(selector)
                    if price_element:
                        price_text = await price_element.inner_text()
                        price = self._extract_price(price_text)
                        if price:
                            break
                except:
                    continue
            
            if not price:
                return None
            
            # –°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–æ–≤–∞—Ä
            link_selectors = [
                'a[href]',
                '[data-testid="product-link"]'
            ]
            
            url = None
            for selector in link_selectors:
                try:
                    link_element = await product_element.query_selector(selector)
                    if link_element:
                        href = await link_element.get_attribute('href')
                        if href:
                            url = urljoin(self.base_url, href)
                            break
                except:
                    continue
            
            # –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –≤–æ–∑–º–æ–∂–Ω–æ –Ω—É–∂–Ω–æ –∫–ª–∏–∫–Ω—É—Ç—å –Ω–∞ –∫–∞—Ä—Ç–æ—á–∫—É
            if not url:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π URL
                    current_url = page.url
                    
                    # –ö–ª–∏–∫–∞–µ–º –Ω–∞ –∫–∞—Ä—Ç–æ—á–∫—É
                    await product_element.click()
                    await asyncio.sleep(1)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ URL
                    new_url = page.url
                    if new_url != current_url:
                        url = new_url
                        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –Ω–∞–∑–∞–¥
                        await page.go_back()
                        await asyncio.sleep(1)
                except:
                    pass
            
            if not url:
                return None
            
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_selectors = [
                'img[src]',
                '[data-testid="product-image"] img',
                '.product-image img'
            ]
            
            photo_url = None
            for selector in image_selectors:
                try:
                    img_element = await product_element.query_selector(selector)
                    if img_element:
                        src = await img_element.get_attribute('src')
                        if src and 'data:' not in src:  # –ò—Å–∫–ª—é—á–∞–µ–º base64 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                            if src.startswith('//'):
                                photo_url = f"https:{src}"
                            elif src.startswith('/'):
                                photo_url = urljoin(self.base_url, src)
                            else:
                                photo_url = src
                            break
                except:
                    continue
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –≤–µ—Å –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
            portion_g = self._extract_weight_from_text(name)
            if not portion_g:
                # –ò—â–µ–º –≤–µ—Å –≤ –¥—Ä—É–≥–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö –∫–∞—Ä—Ç–æ—á–∫–∏
                weight_text = await self._extract_text_from_element(product_element, [
                    '[class*="weight"]', '[class*="Weight"]', 
                    '[class*="gram"]', '[class*="Gram"]',
                    '.weight', '.portion'
                ])
                if weight_text:
                    portion_g = self._extract_weight_from_text(weight_text)
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤–∏–¥–∏–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            all_text = await product_element.inner_text() if product_element else ""
            
            # –ò—â–µ–º —Ç–µ–≥–∏/–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ç–µ–∫—Å—Ç–µ
            tags = self._extract_tags_from_text(all_text)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID –∏–∑ URL
            native_id = self._extract_id_from_url(url)
            
            return {
                'native_id': native_id,
                'name': name.strip(),
                'category': category,
                'price': price,
                'url': url,
                'photo_url': photo_url,
                'portion_g': portion_g,
                'tags': tags,
                'composition': None,
                'kcal_100g': None,
                'protein_100g': None,
                'fat_100g': None,
                'carb_100g': None
            }
            
        except Exception as e:
            logger.warning(f"Failed to extract product data: {e}")
            return None
    
    async def _enrich_item_details(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–∞ –ø–µ—Ä–µ—Ö–æ–¥–æ–º –≤ –∫–∞—Ä—Ç–æ—á–∫—É."""
        page = await self.context.new_page()
        
        try:
            logger.debug(f"Enriching item: {item_data.get('name')}")
            
            await page.goto(item_data['url'], wait_until="networkidle")
            await asyncio.sleep(3)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            
            # –°–æ—Å—Ç–∞–≤/–∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã
            composition = await self._extract_composition(page)
            
            # –ü–∏—â–µ–≤–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å
            nutrition_data = await self._extract_nutrition_info(page)
            
            # –í–µ—Å –ø–æ—Ä—Ü–∏–∏
            portion_g = await self._extract_portion_weight(page)
            
            # –¢–µ–≥–∏/–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
            tags = await self._extract_tags(page)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            brand = await self._extract_brand(page)
            barcode = await self._extract_barcode(page)
            shelf_life = await self._extract_shelf_life(page)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
            item_data.update({
                'composition': composition,
                'portion_g': portion_g,
                'tags': tags,
                'brand': brand,
                'barcode': barcode,
                'shelf_life': shelf_life,
                **nutrition_data
            })
            
        except Exception as e:
            logger.warning(f"Failed to enrich item {item_data.get('url')}: {e}")
        finally:
            await page.close()
        
        return item_data
    
    async def _extract_composition(self, page: Page) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ—Å—Ç–∞–≤–∞."""
        composition_selectors = [
            '[data-testid="composition"]',
            '[data-testid="ingredients"]',
            '.composition',
            '.ingredients',
            '*:has-text("–°–æ—Å—Ç–∞–≤")',
            '*:has-text("–ò–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã")'
        ]
        
        for selector in composition_selectors:
            try:
                if ':has-text(' in selector:
                    # –î–ª—è —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–º –∏—â–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π —ç–ª–µ–º–µ–Ω—Ç
                    elements = await page.query_selector_all(selector)
                    for element in elements:
                        parent = await element.query_selector('xpath=..')
                        if parent:
                            text = await parent.inner_text()
                            if '—Å–æ—Å—Ç–∞–≤' in text.lower() or '–∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç' in text.lower():
                                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å —Å —Å–æ—Å—Ç–∞–≤–æ–º
                                lines = text.split('\n')
                                for i, line in enumerate(lines):
                                    if '—Å–æ—Å—Ç–∞–≤' in line.lower() or '–∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç' in line.lower():
                                        composition_lines = lines[i+1:]
                                        composition = '\n'.join(composition_lines).strip()
                                        if composition:
                                            return composition
                else:
                    element = await page.query_selector(selector)
                    if element:
                        text = await element.inner_text()
                        if text and text.strip():
                            return text.strip()
            except:
                continue
        
        return None
    
    async def _extract_nutrition_info(self, page: Page) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–∏—â–µ–≤–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏."""
        nutrition_data = {}
        
        # –ò—â–µ–º –±–ª–æ–∫ —Å –ø–∏—â–µ–≤–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç—å—é
        nutrition_selectors = [
            '[data-testid="nutrition"]',
            '[data-testid="nutritional-value"]',
            '.nutrition',
            '.nutritional-value',
            '.energy-value',
            '*:has-text("–ü–∏—â–µ–≤–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å")',
            '*:has-text("–≠–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å")',
            '*:has-text("–Ω–∞ 100")'
        ]
        
        nutrition_text = ""
        
        for selector in nutrition_selectors:
            try:
                if ':has-text(' in selector:
                    elements = await page.query_selector_all(selector)
                    for element in elements:
                        parent = await element.query_selector('xpath=..')
                        if parent:
                            text = await parent.inner_text()
                            if any(keyword in text.lower() for keyword in ['–ø–∏—â–µ–≤–∞—è', '—ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∞—è', '–∫–∞–ª–æ—Ä', '–±–µ–ª–∫', '–∂–∏—Ä', '—É–≥–ª–µ–≤–æ–¥']):
                                nutrition_text += text + "\n"
                else:
                    element = await page.query_selector(selector)
                    if element:
                        text = await element.inner_text()
                        if text:
                            nutrition_text += text + "\n"
            except:
                continue
        
        if not nutrition_text:
            # –ò—â–µ–º –≤ –æ–±—â–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            try:
                page_content = await page.content()
                nutrition_text = page_content
            except:
                pass
        
        if nutrition_text:
            # –ü–∞—Ä—Å–∏–º –Ω—É—Ç—Ä–∏–µ–Ω—Ç—ã
            
            # –ö–∞–ª–æ—Ä–∏–∏
            kcal_patterns = [
                r'(\d+(?:[.,]\d+)?)\s*(?:–∫–∫–∞–ª|kcal)',
                r'–∫–∞–ª–æ—Ä.*?(\d+(?:[.,]\d+)?)',
                r'—ç–Ω–µ—Ä–≥–µ—Ç.*?(\d+(?:[.,]\d+)?)'
            ]
            
            for pattern in kcal_patterns:
                match = re.search(pattern, nutrition_text, re.IGNORECASE)
                if match:
                    nutrition_data['kcal_100g'] = Decimal(match.group(1).replace(',', '.'))
                    break
            
            # –ë–µ–ª–∫–∏
            protein_patterns = [
                r'–±–µ–ª–∫[–∏|–∞].*?(\d+(?:[.,]\d+)?)',
                r'(\d+(?:[.,]\d+)?)\s*–≥.*–±–µ–ª–∫',
                r'protein.*?(\d+(?:[.,]\d+)?)'
            ]
            
            for pattern in protein_patterns:
                match = re.search(pattern, nutrition_text, re.IGNORECASE)
                if match:
                    nutrition_data['protein_100g'] = Decimal(match.group(1).replace(',', '.'))
                    break
            
            # –ñ–∏—Ä—ã
            fat_patterns = [
                r'–∂–∏—Ä[—ã|–∞].*?(\d+(?:[.,]\d+)?)',
                r'(\d+(?:[.,]\d+)?)\s*–≥.*–∂–∏—Ä',
                r'fat.*?(\d+(?:[.,]\d+)?)'
            ]
            
            for pattern in fat_patterns:
                match = re.search(pattern, nutrition_text, re.IGNORECASE)
                if match:
                    nutrition_data['fat_100g'] = Decimal(match.group(1).replace(',', '.'))
                    break
            
            # –£–≥–ª–µ–≤–æ–¥—ã
            carb_patterns = [
                r'—É–≥–ª–µ–≤–æ–¥[—ã|–∞].*?(\d+(?:[.,]\d+)?)',
                r'(\d+(?:[.,]\d+)?)\s*–≥.*—É–≥–ª–µ–≤–æ–¥',
                r'carb.*?(\d+(?:[.,]\d+)?)'
            ]
            
            for pattern in carb_patterns:
                match = re.search(pattern, nutrition_text, re.IGNORECASE)
                if match:
                    nutrition_data['carb_100g'] = Decimal(match.group(1).replace(',', '.'))
                    break
        
        return nutrition_data
    
    async def _extract_portion_weight(self, page: Page) -> Optional[Decimal]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–µ—Å–∞ –ø–æ—Ä—Ü–∏–∏."""
        # –ò—â–µ–º –≤–µ—Å –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
        weight_selectors = [
            '[data-testid="weight"]',
            '.weight',
            '.portion-weight',
            '*:has-text("–í–µ—Å")',
            '*:has-text("–ú–∞—Å—Å–∞")',
            '*:has-text(" –≥")'
        ]
        
        for selector in weight_selectors:
            try:
                if ':has-text(' in selector:
                    elements = await page.query_selector_all(selector)
                    for element in elements:
                        text = await element.inner_text()
                        weight_match = re.search(r'(\d+(?:[.,]\d+)?)\s*–≥', text)
                        if weight_match:
                            return Decimal(weight_match.group(1).replace(',', '.'))
                else:
                    element = await page.query_selector(selector)
                    if element:
                        text = await element.inner_text()
                        weight_match = re.search(r'(\d+(?:[.,]\d+)?)\s*–≥', text)
                        if weight_match:
                            return Decimal(weight_match.group(1).replace(',', '.'))
            except:
                continue
        
        # –ò—â–µ–º –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        try:
            title = await page.title()
            weight_match = re.search(r'(\d+)\s*–≥', title)
            if weight_match:
                return Decimal(weight_match.group(1))
        except:
            pass
        
        return None
    
    async def _extract_tags(self, page: Page) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ —Ç–æ–≤–∞—Ä–∞."""
        tags = []
        
        # –ò—â–µ–º —Ç–µ–≥–∏ –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö
        tag_selectors = [
            '[data-testid="tags"]',
            '[data-testid="labels"]',
            '.tags',
            '.labels',
            '.badges'
        ]
        
        for selector in tag_selectors:
            try:
                elements = await page.query_selector_all(f'{selector} *')
                for element in elements:
                    text = await element.inner_text()
                    if text and len(text.strip()) > 1:
                        tags.append(text.strip().lower())
            except:
                continue
        
        # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º
        try:
            page_content = await page.content()
            keywords = ['–æ—Å—Ç—Ä–æ–µ', '–æ—Å—Ç—Ä—ã–π', '–≤–µ–≥–µ—Ç–∞—Ä–∏–∞–Ω—Å–∫–∏–π', '–≤–µ–≥–∞–Ω', '–ø–ø', '–¥–∏–µ—Ç–∏—á–µ—Å–∫–∏–π', 
                       '–±–µ–∑ –≥–ª—é—Ç–µ–Ω–∞', '–±–µ–∑ –ª–∞–∫—Ç–æ–∑—ã', '–æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–π', '—ç–∫–æ', 'bio']
            for keyword in keywords:
                if keyword in page_content.lower():
                    tags.append(keyword)
        except:
            pass
        
        return list(set(tags))
    
    async def _extract_brand(self, page: Page) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±—Ä–µ–Ω–¥–∞."""
        brand_selectors = [
            '[data-testid="brand"]',
            '.brand',
            '.manufacturer',
            '*:has-text("–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å")',
            '*:has-text("–ë—Ä–µ–Ω–¥")'
        ]
        
        return await self._extract_text_by_selectors(page, brand_selectors)
    
    async def _extract_barcode(self, page: Page) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —à—Ç—Ä–∏—Ö–∫–æ–¥–∞."""
        barcode_selectors = [
            '[data-testid="barcode"]',
            '.barcode',
            '*:has-text("–®—Ç—Ä–∏—Ö–∫–æ–¥")'
        ]
        
        return await self._extract_text_by_selectors(page, barcode_selectors)
    
    async def _extract_shelf_life(self, page: Page) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ä–æ–∫–∞ –≥–æ–¥–Ω–æ—Å—Ç–∏."""
        shelf_life_selectors = [
            '[data-testid="shelf-life"]',
            '.shelf-life',
            '*:has-text("–°—Ä–æ–∫ –≥–æ–¥–Ω–æ—Å—Ç–∏")',
            '*:has-text("–ì–æ–¥–µ–Ω –¥–æ")'
        ]
        
        return await self._extract_text_by_selectors(page, shelf_life_selectors)
    
    async def _extract_text_by_selectors(self, page: Page, selectors: List[str]) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å–ø–∏—Å–∫—É —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤."""
        for selector in selectors:
            try:
                if ':has-text(' in selector:
                    elements = await page.query_selector_all(selector)
                    for element in elements:
                        parent = await element.query_selector('xpath=..')
                        if parent:
                            text = await parent.inner_text()
                            if text and text.strip():
                                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é —á–∞—Å—Ç—å
                                lines = text.split('\n')
                                for line in lines:
                                    if len(line.strip()) > 2 and not any(word in line.lower() for word in ['–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', '–±—Ä–µ–Ω–¥', '—à—Ç—Ä–∏—Ö–∫–æ–¥', '—Å—Ä–æ–∫']):
                                        return line.strip()
                else:
                    element = await page.query_selector(selector)
                    if element:
                        text = await element.inner_text()
                        if text and text.strip():
                            return text.strip()
            except:
                continue
        return None
    
    def _extract_price(self, price_text: str) -> Optional[Decimal]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        if not price_text:
            return None
        
        # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä, —Ç–æ—á–µ–∫ –∏ –∑–∞–ø—è—Ç—ã—Ö
        clean_price = re.sub(r'[^\d.,]', '', price_text)
        
        # –ò—â–µ–º —á–∏—Å–ª–æ
        price_match = re.search(r'(\d+(?:[.,]\d+)?)', clean_price)
        if price_match:
            price_str = price_match.group(1).replace(',', '.')
            try:
                return Decimal(price_str)
            except:
                pass
        
        return None
    
    def _extract_weight_from_text(self, text: str) -> Optional[Decimal]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–µ—Å–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        if not text:
            return None
        
        # –ò—â–µ–º –≤–µ—Å –≤ –≥—Ä–∞–º–º–∞—Ö
        weight_patterns = [
            r'(\d+(?:[.,]\d+)?)\s*–≥\b',
            r'(\d+(?:[.,]\d+)?)\s*–≥—Ä\b',
            r'(\d+(?:[.,]\d+)?)\s*gram\b',
            r'(\d+(?:[.,]\d+)?)\s*g\b'
        ]
        
        for pattern in weight_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    return Decimal(match.group(1).replace(',', '.'))
                except:
                    continue
        
        return None
    
    def _extract_tags_from_text(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        if not text:
            return []
        
        tags = []
        text_lower = text.lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ç–µ–≥–æ–≤
        tag_keywords = {
            '–æ—Å—Ç—Ä–æ–µ': ['–æ—Å—Ç—Ä—ã–π', '–æ—Å—Ç—Ä–∞—è', '–æ—Å—Ç—Ä–æ–µ', '–ø–µ—Ä–µ—Ü', '—á–∏–ª–∏'],
            '–≤–µ–≥–µ—Ç–∞—Ä–∏–∞–Ω—Å–∫–æ–µ': ['–≤–µ–≥–µ—Ç–∞—Ä–∏–∞–Ω—Å–∫–∏–π', '–≤–µ–≥–µ—Ç–∞—Ä–∏–∞–Ω—Å–∫–∞—è', '–≤–µ–≥–∞–Ω'],
            '–¥–∏–µ—Ç–∏—á–µ—Å–∫–æ–µ': ['–¥–∏–µ—Ç–∏—á–µ—Å–∫–∏–π', '–¥–∏–µ—Ç–∏—á–µ—Å–∫–∞—è', '–ø–ø', '—Ñ–∏—Ç–Ω–µ—Å'],
            '–±–µ–∑ –≥–ª—é—Ç–µ–Ω–∞': ['–±–µ–∑ –≥–ª—é—Ç–µ–Ω–∞', '–±–µ–∑–≥–ª—é—Ç–µ–Ω–æ–≤—ã–π'],
            '–±–µ–∑ –ª–∞–∫—Ç–æ–∑—ã': ['–±–µ–∑ –ª–∞–∫—Ç–æ–∑—ã', '–±–µ–∑–ª–∞–∫—Ç–æ–∑–Ω—ã–π'],
            '–æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–æ–µ': ['–æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏–π', '—ç–∫–æ', '–±–∏–æ'],
            '–¥–æ–º–∞—à–Ω–µ–µ': ['–¥–æ–º–∞—à–Ω–∏–π', '–¥–æ–º–∞—à–Ω—è—è', '—Ñ–µ—Ä–º–µ—Ä—Å–∫–∏–π']
        }
        
        for tag, keywords in tag_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                tags.append(tag)
        
        return tags
    
    async def _extract_text_from_element(self, element, selectors: List[str]) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º."""
        for selector in selectors:
            try:
                sub_element = await element.query_selector(selector)
                if sub_element:
                    text = await sub_element.inner_text()
                    if text and text.strip():
                        return text.strip()
            except:
                continue
        return None
    
    def _extract_id_from_url(self, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID —Ç–æ–≤–∞—Ä–∞ –∏–∑ URL."""
        # –ò—â–µ–º ID –≤ URL –õ–∞–≤–∫–∏
        id_patterns = [
            r'/product/([^/?]+)',
            r'/([^/?]+)/?$',
            r'id=([^&]+)'
        ]
        
        for pattern in id_patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        # –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ö–µ—à –æ—Ç URL
        return super()._generate_id_from_url(url)
    
    async def _smart_scroll_to_load_all(self, page) -> None:
        """–£–º–Ω–∞—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∞ —Å –æ–∂–∏–¥–∞–Ω–∏–µ–º –ø–æ—è–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –∫–∞—Ä—Ç–æ—á–µ–∫."""
        logger.info("üîÑ Smart scrolling to load all products...")
        
        prev_count = 0
        stable_rounds = 0
        max_rounds = 20
        
        for round_num in range(max_rounds):
            # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –≤–Ω–∏–∑
            await page.mouse.wheel(0, 4000)
            await asyncio.sleep(1)
            
            # –°—á–∏—Ç–∞–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏
            try:
                grid = page.locator("[class*='Card'], [class*='Product'], .product-card")
                current_count = await grid.count()
                
                logger.debug(f"   Round {round_num + 1}: {current_count} cards")
                
                if current_count > prev_count:
                    prev_count = current_count
                    stable_rounds = 0
                    logger.debug(f"   ‚úÖ New cards loaded: {current_count}")
                else:
                    stable_rounds += 1
                    logger.debug(f"   ‚è∏Ô∏è No new cards: stable round {stable_rounds}")
                
                # –ï—Å–ª–∏ 3 —Ä–∞—É–Ω–¥–∞ –±–µ–∑ –Ω–æ–≤—ã—Ö –∫–∞—Ä—Ç–æ—á–µ–∫ - –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
                if stable_rounds >= 3:
                    logger.info(f"‚úÖ Scrolling completed: {current_count} cards loaded")
                    break
                    
            except Exception as e:
                logger.warning(f"Error during scroll round {round_num + 1}: {e}")
                break
        
        logger.info(f"üèÅ Smart scrolling finished after {round_num + 1} rounds")
