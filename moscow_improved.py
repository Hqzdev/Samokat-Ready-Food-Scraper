#!/usr/bin/env python3
"""
üèóÔ∏è –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–ê–†–°–ï–† –í–ö–£–°–í–ò–õ–õ–ê –î–õ–Ø –ú–û–°–ö–í–´
–ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤.
"""

import asyncio
import csv
import json
import logging
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from urllib.parse import urljoin, quote

try:
    from selectolax.parser import HTMLParser
except ImportError:
    HTMLParser = None

import httpx


class AntiBotClient:
    """–ü—Ä–æ—Å—Ç–æ–π HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã."""
    
    def __init__(self, concurrency: int = 10, timeout: int = 30):
        self.semaphore = asyncio.Semaphore(concurrency)
        self.timeout = timeout
        
    async def request(self, method: str, url: str, **kwargs):
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å HTTP –∑–∞–ø—Ä–æ—Å."""
        async with self.semaphore:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            async with httpx.AsyncClient(timeout=self.timeout, headers=headers) as client:
                response = await client.request(method, url, **kwargs)
                return response
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –∫–ª–∏–µ–Ω—Ç–∞."""
        pass


class VkusvillImprovedParser:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π."""
    
    def __init__(self, antibot_client):
        self.antibot_client = antibot_client
        self.BASE_URL = "https://vkusvill.ru"
        
    async def scrape_all_products(self, limit: int = 1500) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π."""
        print(f"üèóÔ∏è –ù–∞—á–∏–Ω–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–∞ {limit} —Ç–æ–≤–∞—Ä–æ–≤...")
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ª–æ–∫–∞—Ü–∏–∏ –¥–ª—è –ú–æ—Å–∫–≤—ã
        await self._set_location()
        
        # –°–±–æ—Ä —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π
        print("üìã –°–æ–±–∏—Ä–∞–µ–º –í–°–ï —Ç–æ–≤–∞—Ä—ã –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π...")
        product_urls = set()
        
        ready_food_categories = [
            "/goods/gotovaya-eda/",
            "/goods/gotovaya-eda/novinki/",
            "/goods/gotovaya-eda/vtorye-blyuda/",
            "/goods/gotovaya-eda/salaty/",
            "/goods/gotovaya-eda/sendvichi-shaurma-i-burgery/",
            "/goods/gotovaya-eda/supy/",
            "/goods/gotovaya-eda/zavtraki/",
            "/goods/gotovaya-eda/zakuski/",
            "/goods/gotovaya-eda/rolly-i-sety/",
            "/goods/gotovaya-eda/onigiri/",
            "/goods/gotovaya-eda/pirogi-pirozhki-i-lepyeshki/"
        ]
        
        for category in ready_food_categories:
            try:
                urls = await self._get_all_category_products(category)
                product_urls.update(urls)
                print(f"   {category}: +{len(urls)} —Ç–æ–≤–∞—Ä–æ–≤")
            except Exception as e:
                print(f"   ‚ùå {category}: {e}")
        
        print(f"üì¶ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(product_urls)} —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–æ–≤–∞—Ä—ã")
        
        # –ü–∞—Ä—Å–∏–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã
        products = []
        product_list = list(product_urls)
        
        batch_size = 10
        semaphore = asyncio.Semaphore(8)
        
        for i in range(0, len(product_list), batch_size):
            batch = product_list[i:i + batch_size]
            print(f"üîç –¢–æ–≤–∞—Ä—ã {i+1}-{min(i+batch_size, len(product_list))}/{len(product_list)}")
            
            async def process_product(url):
                async with semaphore:
                    return await self._extract_full_product(url)
            
            tasks = [process_product(url) for url in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, dict) and result:
                    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –í–°–ï —Ç–æ–≤–∞—Ä—ã –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞ –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã
                    products.append(result)
            
            await asyncio.sleep(0.5)
        
        print(f"üèÅ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(products)} —Ç–æ–≤–∞—Ä–æ–≤")
        return products
    
    async def _set_location(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ª–æ–∫–∞—Ü–∏–∏ –¥–ª—è –ú–æ—Å–∫–≤—ã."""
        try:
            location_url = f"{self.BASE_URL}/api/location?city=–ú–æ—Å–∫–≤–∞&lat=55.7558&lon=37.6176"
            await self.antibot_client.request(method="GET", url=location_url)
            print("üìç –õ–æ–∫–∞—Ü–∏—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: –ú–æ—Å–∫–≤–∞ (—Ü–µ–Ω—Ç—Ä)")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –ª–æ–∫–∞—Ü–∏–∏: {e}")
    
    async def _get_all_category_products(self, category: str) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –í–°–ï —Ç–æ–≤–∞—Ä—ã –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —á–µ—Ä–µ–∑ –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã."""
        product_urls = set()
        
        # –ú–µ—Ç–æ–¥ 1: –û–±—ã—á–Ω–∞—è –ø–∞–≥–∏–Ω–∞—Ü–∏—è —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –ª–∏–º–∏—Ç–æ–º —Å—Ç—Ä–∞–Ω–∏—Ü
        print(f"      üîç –ü–∞–≥–∏–Ω–∞—Ü–∏—è –¥–ª—è {category}")
        page = 1
        consecutive_empty = 0
        
        while page <= 100 and consecutive_empty < 5:  # –î–æ 100 —Å—Ç—Ä–∞–Ω–∏—Ü, –Ω–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –ø–æ—Å–ª–µ 5 –ø—É—Å—Ç—ã—Ö –ø–æ–¥—Ä—è–¥
            try:
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã URL –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
                urls_to_try = [
                    f"{self.BASE_URL}{category}?page={page}",
                    f"{self.BASE_URL}{category}?PAGEN_1={page}",
                    f"{self.BASE_URL}{category}?p={page}",
                    f"{self.BASE_URL}{category}?offset={24*(page-1)}&limit=24",
                    f"{self.BASE_URL}{category}?start={24*(page-1)}&count=24"
                ]
                
                found_products = False
                
                for url in urls_to_try:
                    try:
                        response = await self.antibot_client.request(method="GET", url=url)
                        
                        if response.status_code != 200:
                            continue
                            
                        parser = HTMLParser(response.text)
                        links = parser.css('a[href*="/goods/"][href$=".html"]')
                        
                        if links:
                            page_count = 0
                            for link in links:
                                href = link.attributes.get('href')
                                if href and '.html' in href and '/goods/' in href:
                                    full_url = urljoin(self.BASE_URL, href)
                                    if full_url not in product_urls:
                                        product_urls.add(full_url)
                                        page_count += 1
                            
                            if page_count > 0:
                                print(f"         ‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: +{page_count} —Ç–æ–≤–∞—Ä–æ–≤ (–≤—Å–µ–≥–æ: {len(product_urls)})")
                                found_products = True
                                consecutive_empty = 0
                                break
                    
                    except Exception:
                        continue
                
                if not found_products:
                    consecutive_empty += 1
                    print(f"         ‚ùå –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –ø—É—Å—Ç–∞—è ({consecutive_empty}/5)")
                
                page += 1
                await asyncio.sleep(0.1)
                
            except Exception:
                consecutive_empty += 1
                page += 1
        
        # –ú–µ—Ç–æ–¥ 2: –ü–æ–ø—ã—Ç–∫–∞ AJAX –∑–∞–≥—Ä—É–∑–∫–∏
        if len(product_urls) < 200:  # –ï—Å–ª–∏ –º–∞–ª–æ —Ç–æ–≤–∞—Ä–æ–≤, –ø—Ä–æ–±—É–µ–º AJAX
            print(f"      üîÑ AJAX –∑–∞–≥—Ä—É–∑–∫–∞ –¥–ª—è {category}")
            await self._try_ajax_load(category, product_urls)
        
        # –ú–µ—Ç–æ–¥ 3: –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
        if len(product_urls) < 200:
            print(f"      üîÑ –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –¥–ª—è {category}")
            sort_params = ['price', 'name', 'popular', 'new', 'rating']
            for sort_param in sort_params:
                try:
                    url = f"{self.BASE_URL}{category}?sort={sort_param}"
                    response = await self.antibot_client.request(method="GET", url=url)
                    
                    if response.status_code == 200:
                        parser = HTMLParser(response.text)
                        links = parser.css('a[href*="/goods/"][href$=".html"]')
                        
                        sort_count = 0
                        for link in links:
                            href = link.attributes.get('href')
                            if href and '.html' in href and '/goods/' in href:
                                full_url = urljoin(self.BASE_URL, href)
                                if full_url not in product_urls:
                                    product_urls.add(full_url)
                                    sort_count += 1
                        
                        if sort_count > 0:
                            print(f"         ‚úÖ –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ {sort_param}: +{sort_count} —Ç–æ–≤–∞—Ä–æ–≤")
                        
                        await asyncio.sleep(0.2)
                        
                except Exception:
                    continue
        
        return list(product_urls)
    
    async def _try_ajax_load(self, category: str, product_urls: set):
        """–ü–æ–ø—ã—Ç–∫–∞ AJAX –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤."""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            base_url = f"{self.BASE_URL}{category}"
            response = await self.antibot_client.request(method="GET", url=base_url)
            
            if response.status_code != 200:
                return
            
            # –ò—â–µ–º AJAX endpoints –≤ JavaScript –∫–æ–¥–µ
            ajax_patterns = [
                r'url\s*:\s*["\']([^"\']*load[^"\']*)["\']',
                r'["\']([^"\']*ajax[^"\']*load[^"\']*)["\']',
                r'["\']([^"\']*catalog[^"\']*load[^"\']*)["\']'
            ]
            
            ajax_urls = set()
            for pattern in ajax_patterns:
                matches = re.findall(pattern, response.text, re.I)
                for match in matches:
                    if match.startswith('/'):
                        ajax_urls.add(self.BASE_URL + match)
                    elif match.startswith('http'):
                        ajax_urls.add(match)
            
            # –ü—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ AJAX endpoints
            standard_endpoints = [
                f"{self.BASE_URL}/ajax/catalog/",
                f"{self.BASE_URL}/ajax/goods/",
                f"{self.BASE_URL}/local/ajax/catalog.php",
                f"{self.BASE_URL}/bitrix/components/custom/catalog.section/ajax.php"
            ]
            ajax_urls.update(standard_endpoints)
            
            # –ü—Ä–æ–±—É–µ–º –∫–∞–∂–¥—ã–π endpoint
            for ajax_url in list(ajax_urls)[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
                try:
                    # POST –∑–∞–ø—Ä–æ—Å —Å –¥–∞–Ω–Ω—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                    data = {
                        'PAGEN_1': 2,
                        'SECTION_CODE': category.strip('/').split('/')[-1],
                        'action': 'load_more',
                        'page': 2
                    }
                    
                    response = await self.antibot_client.request(
                        method="POST",
                        url=ajax_url,
                        data=data,
                        headers={
                            'Content-Type': 'application/x-www-form-urlencoded',
                            'X-Requested-With': 'XMLHttpRequest',
                            'Referer': base_url
                        }
                    )
                    
                    if response.status_code == 200 and len(response.text) > 100:
                        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ç–æ–≤–∞—Ä—ã –≤ –æ—Ç–≤–µ—Ç–µ
                        if 'goods/' in response.text and '.html' in response.text:
                            parser = HTMLParser(response.text)
                            links = parser.css('a[href*="/goods/"][href$=".html"]')
                            
                            ajax_count = 0
                            for link in links:
                                href = link.attributes.get('href')
                                if href and '.html' in href and '/goods/' in href:
                                    full_url = urljoin(self.BASE_URL, href)
                                    if full_url not in product_urls:
                                        product_urls.add(full_url)
                                        ajax_count += 1
                            
                            if ajax_count > 0:
                                print(f"         ‚úÖ AJAX {ajax_url}: +{ajax_count} —Ç–æ–≤–∞—Ä–æ–≤")
                                return  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ä–∞–±–æ—á–∏–π endpoint, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
                
                except Exception:
                    continue
                    
        except Exception:
            pass
    
    def _is_ready_food(self, product: Dict) -> bool:
        """–ü—Ä–∏–Ω–∏–º–∞–µ–º –í–°–ï —Ç–æ–≤–∞—Ä—ã –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏."""
        return True
    
    async def _extract_full_product(self, url: str, retry_count: int = 0) -> Optional[Dict]:
        """–ü–æ–ª–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ —Å retry –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞."""
        max_retries = 3
        
        try:
            response = await self.antibot_client.request(method="GET", url=url)
            if response.status_code != 200 or not HTMLParser:
                return None
                
            parser = HTMLParser(response.text)
            page_text = response.text
            
            # –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            product = {
                'id': self._extract_id(url),
                'name': self._extract_name(parser, page_text),
                'price': self._extract_price_with_retry(parser, page_text),
                'category': '–ì–æ—Ç–æ–≤–∞—è –µ–¥–∞',
                'url': url,
                'shop': 'vkusvill_improved',
                'photo': self._extract_photo(parser),
                'composition': self._extract_composition_with_retry(parser, page_text),
                'tags': '',
                'portion_g': self._extract_portion_weight(parser, page_text)
            }
            
            # –ë–ñ–£ —Å retry
            nutrition = self._extract_bju_with_retry(parser, page_text)
            product.update(nutrition)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
            filled_bju = sum(1 for field in ['kcal_100g', 'protein_100g', 'fat_100g', 'carb_100g'] if product.get(field))
            has_composition = bool(product.get('composition'))
            has_price = bool(product.get('price'))
            
            # Retry –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ–ø–æ–ª–Ω—ã–µ
            if retry_count < max_retries:
                need_retry = False
                
                if filled_bju < 3:  # –ú–∞–ª–æ –ë–ñ–£
                    need_retry = True
                    print(f"      üîÑ RETRY #{retry_count + 1} - –º–∞–ª–æ –ë–ñ–£ ({filled_bju}/4)")
                elif not has_composition:  # –ù–µ—Ç —Å–æ—Å—Ç–∞–≤–∞
                    need_retry = True
                    print(f"      üîÑ RETRY #{retry_count + 1} - –Ω–µ—Ç —Å–æ—Å—Ç–∞–≤–∞")
                elif not has_price:  # –ù–µ—Ç —Ü–µ–Ω—ã
                    need_retry = True
                    print(f"      üîÑ RETRY #{retry_count + 1} - –Ω–µ—Ç —Ü–µ–Ω—ã")
                
                if need_retry:
                    await asyncio.sleep(0.5 + retry_count * 0.5)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–∞—É–∑—É
                    return await self._extract_full_product(url, retry_count + 1)
            
            # –õ–æ–≥–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            quality_score = (filled_bju + (1 if has_composition else 0) + (1 if has_price else 0)) / 6 * 100
            print(f"      üì¶ {product['name'][:35]}... –ë–ñ–£:{filled_bju}/4 –°–æ—Å—Ç–∞–≤:{'‚úì' if has_composition else '‚úó'} –¶–µ–Ω–∞:{'‚úì' if has_price else '‚úó'} –ö–∞—á–µ—Å—Ç–≤–æ:{quality_score:.0f}%")
            
            if not product['name']:
                return None
            
            return product
            
        except Exception:
            if retry_count < max_retries:
                await asyncio.sleep(1)
                return await self._extract_full_product(url, retry_count + 1)
            return None
    
    def _extract_id(self, url: str) -> str:
        """ID —Ç–æ–≤–∞—Ä–∞ –∏–∑ URL."""
        match = re.search(r'/goods/([^/]+)\.html', url)
        return match.group(1) if match else str(hash(url))[-8:]
    
    def _extract_name(self, parser, page_text: str) -> str:
        """–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞."""
        selectors = ['h1', '.product-title', '.goods-title']
        for selector in selectors:
            element = parser.css_first(selector)
            if element and element.text(strip=True):
                return element.text(strip=True)[:150]
        return ""
    
    def _extract_price_with_retry(self, parser, page_text: str) -> str:
        """–¶–µ–Ω–∞ —Ç–æ–≤–∞—Ä–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –ø–æ–∏—Å–∫–æ–º."""
        # –ú–µ—Ç–æ–¥ 1: CSS —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        selectors = ['.price', '.product-price', '.cost', '.goods-price', '[class*="price"]', '[data-price]']
        
        for selector in selectors:
            elements = parser.css(selector)
            for element in elements:
                price_text = element.text(strip=True)
                numbers = re.findall(r'(\d+(?:[.,]\d+)?)', price_text)
                for num in numbers:
                    try:
                        price_val = float(num.replace(',', '.'))
                        if 10 <= price_val <= 10000:
                            return num.replace(',', '.')
                    except ValueError:
                        continue
        
        # –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ –≤ JSON –¥–∞–Ω–Ω—ã—Ö
        json_patterns = [
            r'"price"\s*:\s*"?(\d+(?:[.,]\d+)?)"?',
            r'"cost"\s*:\s*"?(\d+(?:[.,]\d+)?)"?',
            r'"currentPrice"\s*:\s*"?(\d+(?:[.,]\d+)?)"?'
        ]
        for pattern in json_patterns:
            match = re.search(pattern, page_text, re.I)
            if match:
                try:
                    price_val = float(match.group(1).replace(',', '.'))
                    if 10 <= price_val <= 10000:
                        return match.group(1).replace(',', '.')
                except ValueError:
                    continue
        
        # –ú–µ—Ç–æ–¥ 3: –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É
        text_patterns = [
            r'(\d+(?:[.,]\d+)?)\s*—Ä—É–±',
            r'(\d+(?:[.,]\d+)?)\s*‚ÇΩ',
            r'—Ü–µ–Ω–∞[:\s]*(\d+(?:[.,]\d+)?)',
            r'—Å—Ç–æ–∏–º–æ—Å—Ç—å[:\s]*(\d+(?:[.,]\d+)?)'
        ]
        for pattern in text_patterns:
            matches = re.finditer(pattern, page_text, re.I)
            for match in matches:
                try:
                    price_val = float(match.group(1).replace(',', '.'))
                    if 10 <= price_val <= 10000:
                        return match.group(1).replace(',', '.')
                except ValueError:
                    continue
        
        return ""
    
    def _extract_photo(self, parser) -> str:
        """–§–æ—Ç–æ —Ç–æ–≤–∞—Ä–∞."""
        selectors = ['img[src*="product"]', 'img[src*="goods"]', 'img']
        
        for selector in selectors:
            elements = parser.css(selector)
            for element in elements:
                src = element.attributes.get('src') or element.attributes.get('data-src')
                if src and any(keyword in src.lower() for keyword in ['product', 'goods', 'catalog']):
                    if not any(skip in src.lower() for skip in ['icon', 'logo', 'banner']):
                        full_url = urljoin(self.BASE_URL, src)
                        if full_url.startswith('http'):
                            return full_url
        
        return ""
    
    def _extract_composition_with_retry(self, parser, page_text: str) -> str:
        """–°–æ—Å—Ç–∞–≤ —Ç–æ–≤–∞—Ä–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –ø–æ–∏—Å–∫–æ–º."""
        # –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É "–°–æ—Å—Ç–∞–≤"
        elements = parser.css('div, p, span, td, li, section, article')
        for element in elements:
            text = element.text().strip()
            text_lower = text.lower()
            
            if '—Å–æ—Å—Ç–∞–≤' in text_lower and len(text) > 10:
                if not any(word in text_lower for word in ['–º–µ–Ω—é', '–∫–∞—Ç–∞–ª–æ–≥', '–∫–æ—Ä–∑–∏–Ω–∞', '–Ω–∞–≤–∏–≥–∞—Ü–∏—è']):
                    if text_lower.startswith('—Å–æ—Å—Ç–∞–≤'):
                        return text[:800]
                    elif len(text) < 800:
                        return text[:500]
        
        # –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ –ø–æ CSS —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
        composition_selectors = [
            '[class*="composition"]', '[class*="ingredients"]', 
            '[data-testid*="composition"]', '.product-composition',
            '.goods-composition', '.ingredients', '[class*="content"]'
        ]
        
        for selector in composition_selectors:
            element = parser.css_first(selector)
            if element:
                text = element.text(strip=True)
                if len(text) > 20 and len(text) < 1000:
                    return text[:500]
        
        # –ú–µ—Ç–æ–¥ 3: –ü–æ–∏—Å–∫ –≤ JSON –¥–∞–Ω–Ω—ã—Ö
        json_patterns = [
            r'"composition"\s*:\s*"([^"]+)"',
            r'"ingredients"\s*:\s*"([^"]+)"',
            r'"description"\s*:\s*"([^"]*)"'
        ]
        for pattern in json_patterns:
            match = re.search(pattern, page_text, re.I)
            if match:
                composition = match.group(1)
                if len(composition) > 20:
                    return composition[:500]
        
        return ""
    
    def _extract_portion_weight(self, parser, page_text: str) -> str:
        """–í–µ—Å –ø–æ—Ä—Ü–∏–∏."""
        patterns = [r'(\d+(?:[.,]\d+)?)\s*(?:–≥|–≥—Ä|–≥—Ä–∞–º)']
        for pattern in patterns:
            matches = re.finditer(pattern, page_text, re.I)
            for match in matches:
                try:
                    weight = float(match.group(1).replace(',', '.'))
                    if 10 <= weight <= 2000:
                        return f"{weight}–≥"
                except ValueError:
                    continue
        return ""
    
    def _extract_bju_with_retry(self, parser, page_text: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ë–ñ–£ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –ø–æ–∏—Å–∫–æ–º."""
        nutrition = {'kcal_100g': '', 'protein_100g': '', 'fat_100g': '', 'carb_100g': ''}
        
        # –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –≤ —ç–ª–µ–º–µ–Ω—Ç–∞—Ö —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        elements = parser.css('div, span, p, td, th, li, section')
        for element in elements:
            text = element.text().lower()
            original_text = element.text()
            
            if any(word in text for word in ['–∫–∫–∞–ª', '–±–µ–ª–∫–∏', '–∂–∏—Ä—ã', '—É–≥–ª–µ–≤–æ–¥—ã', '–∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç—å', '—ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∞—è']):
                # –ö–∞–ª–æ—Ä–∏–∏ - —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
                if ('–∫–∫–∞–ª' in text or '–∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç—å' in text or '—ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∞—è' in text) and not nutrition['kcal_100g']:
                    kcal_patterns = [
                        r'(\d+(?:[.,]\d+)?)\s*–∫–∫–∞–ª',
                        r'–∫–∫–∞–ª[:\s]*(\d+(?:[.,]\d+)?)',
                        r'–∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç—å[:\s]*(\d+(?:[.,]\d+)?)',
                        r'—ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∞—è\s+—Ü–µ–Ω–Ω–æ—Å—Ç—å[:\s]*(\d+(?:[.,]\d+)?)',
                        r'(\d+(?:[.,]\d+)?)\s+–∫–∫–∞–ª'
                    ]
                    for pattern in kcal_patterns:
                        match = re.search(pattern, text)
                        if match:
                            try:
                                val = float(match.group(1).replace(',', '.'))
                                if 10 <= val <= 900:
                                    nutrition['kcal_100g'] = match.group(1).replace(',', '.')
                                    break
                            except ValueError:
                                continue
                
                # –ë–µ–ª–∫–∏ - —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
                if '–±–µ–ª–∫' in text and not nutrition['protein_100g']:
                    protein_patterns = [
                        r'(\d+(?:[.,]\d+)?)\s+–±–µ–ª–∫–∏,\s*–≥',
                        r'–±–µ–ª–∫[–∏–∞][:\s]*(\d+(?:[.,]\d+)?)',
                        r'–±–µ–ª–æ–∫[:\s]*(\d+(?:[.,]\d+)?)',
                        r'–ø—Ä–æ—Ç–µ–∏–Ω[:\s]*(\d+(?:[.,]\d+)?)'
                    ]
                    for pattern in protein_patterns:
                        match = re.search(pattern, text)
                        if match:
                            try:
                                val = float(match.group(1).replace(',', '.'))
                                if 0 <= val <= 100:
                                    nutrition['protein_100g'] = match.group(1).replace(',', '.')
                                    break
                            except ValueError:
                                continue
                
                # –ñ–∏—Ä—ã - —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
                if '–∂–∏—Ä' in text and not nutrition['fat_100g']:
                    fat_patterns = [
                        r'(\d+(?:[.,]\d+)?)\s+–∂–∏—Ä—ã,\s*–≥',
                        r'–∂–∏—Ä[—ã–∞][:\s]*(\d+(?:[.,]\d+)?)',
                        r'–∂–∏—Ä[:\s]*(\d+(?:[.,]\d+)?)'
                    ]
                    for pattern in fat_patterns:
                        match = re.search(pattern, text)
                        if match:
                            try:
                                val = float(match.group(1).replace(',', '.'))
                                if 0 <= val <= 100:
                                    nutrition['fat_100g'] = match.group(1).replace(',', '.')
                                    break
                            except ValueError:
                                continue
                
                # –£–≥–ª–µ–≤–æ–¥—ã - —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
                if '—É–≥–ª–µ–≤–æ–¥' in text and not nutrition['carb_100g']:
                    carb_patterns = [
                        r'(\d+(?:[.,]\d+)?)\s+—É–≥–ª–µ–≤–æ–¥—ã,\s*–≥',
                        r'—É–≥–ª–µ–≤–æ–¥[—ã–∞][:\s]*(\d+(?:[.,]\d+)?)',
                        r'—É–≥–ª–µ–≤–æ–¥[:\s]*(\d+(?:[.,]\d+)?)'
                    ]
                    for pattern in carb_patterns:
                        match = re.search(pattern, text)
                        if match:
                            try:
                                val = float(match.group(1).replace(',', '.'))
                                if 0 <= val <= 100:
                                    nutrition['carb_100g'] = match.group(1).replace(',', '.')
                                    break
                            except ValueError:
                                continue
        
        # –ú–µ—Ç–æ–¥ 2: JSON-LD —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        try:
            blocks = re.findall(r'<script[^>]+type=["\']application/ld\+json["\'][^>]*>(.*?)</script>', page_text, re.S|re.I)
            for raw in blocks:
                try:
                    data = json.loads(raw)
                    
                    def visit(obj):
                        if isinstance(obj, dict):
                            if obj.get('@type') in ('NutritionInformation', 'Nutrition'):
                                kcal = obj.get('calories') or obj.get('energy')
                                protein = obj.get('proteinContent')
                                fat = obj.get('fatContent')
                                carb = obj.get('carbohydrateContent')
                                
                                if kcal and not nutrition['kcal_100g']: 
                                    nutrition['kcal_100g'] = str(kcal)
                                if protein and not nutrition['protein_100g']: 
                                    nutrition['protein_100g'] = str(protein)
                                if fat and not nutrition['fat_100g']: 
                                    nutrition['fat_100g'] = str(fat)
                                if carb and not nutrition['carb_100g']: 
                                    nutrition['carb_100g'] = str(carb)
                            
                            for v in obj.values():
                                visit(v)
                        elif isinstance(obj, list):
                            for v in obj:
                                visit(v)
                    
                    visit(data)
                except:
                    continue
        except:
            pass
        
        # –ú–µ—Ç–æ–¥ 3: –ü–æ–∏—Å–∫ –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö
        try:
            tables = parser.css('table')
            for table in tables:
                rows = table.css('tr')
                for row in rows:
                    cells = row.css('td, th')
                    if len(cells) >= 2:
                        header = cells[0].text().lower()
                        value_text = cells[1].text()
                        
                        num_match = re.search(r'(\d+(?:[.,]\d+)?)', value_text)
                        if num_match:
                            value = num_match.group(1).replace(',', '.')
                            
                            if ('–∫–∫–∞–ª' in header or '–∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç—å' in header) and not nutrition['kcal_100g']:
                                nutrition['kcal_100g'] = value
                            elif '–±–µ–ª–∫' in header and not nutrition['protein_100g']:
                                nutrition['protein_100g'] = value
                            elif '–∂–∏—Ä' in header and not nutrition['fat_100g']:
                                nutrition['fat_100g'] = value
                            elif '—É–≥–ª–µ–≤–æ–¥' in header and not nutrition['carb_100g']:
                                nutrition['carb_100g'] = value
        except:
            pass
        
        return nutrition


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞."""
    limit = int(sys.argv[1]) if len(sys.argv) > 1 else 1500
    
    print("üèóÔ∏è –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–ê–†–°–ï–† –í–ö–£–°–í–ò–õ–õ–ê - –ú–û–°–ö–í–ê")
    print("=" * 60)
    print(f"üéØ –¶–µ–ª—å: {limit} —Ç–æ–≤–∞—Ä–æ–≤ —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
    print("üìç –ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ: –ú–æ—Å–∫–≤–∞")
    print("‚ö° –†–µ–∂–∏–º: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –ø–∞–≥–∏–Ω–∞—Ü–∏—è")
    print()
    
    logging.basicConfig(level=logging.WARNING)
    
    antibot_client = AntiBotClient(concurrency=10, timeout=60)
    
    try:
        parser = VkusvillImprovedParser(antibot_client)
        
        start_time = time.time()
        products = await parser.scrape_all_products(limit)
        end_time = time.time()
        
        if not products:
            print("‚ùå –ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        bju_stats = {'full_bju': 0, 'good_bju': 0, 'some_bju': 0, 'no_bju': 0}
        composition_stats = {'has_composition': 0, 'no_composition': 0}
        
        for product in products:
            bju_fields = ['kcal_100g', 'protein_100g', 'fat_100g', 'carb_100g']
            filled = sum(1 for field in bju_fields if product.get(field))
            has_composition = bool(product.get('composition'))
            
            if filled == 4:
                bju_stats['full_bju'] += 1
            elif filled == 3:
                bju_stats['good_bju'] += 1
            elif filled >= 1:
                bju_stats['some_bju'] += 1
            else:
                bju_stats['no_bju'] += 1
            
            if has_composition:
                composition_stats['has_composition'] += 1
            else:
                composition_stats['no_composition'] += 1
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        timestamp = int(time.time())
        csv_file = f"data/moscow_improved_{timestamp}.csv"
        jsonl_file = f"data/moscow_improved_{timestamp}.jsonl"
        
        Path("data").mkdir(exist_ok=True)
        
        if products:
            fieldnames = list(products[0].keys())
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(products)
        
        with open(jsonl_file, 'w', encoding='utf-8') as f:
            for product in products:
                f.write(json.dumps(product, ensure_ascii=False) + '\n')
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        duration = end_time - start_time
        excellent_quality = bju_stats['full_bju'] + bju_stats['good_bju']
        
        print()
        print("üèÅ –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù")
        print("=" * 60)
        print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"   ‚Ä¢ –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}")
        print(f"   ‚Ä¢ –ü–æ–ª–Ω–æ–µ –ë–ñ–£ (4/4): {bju_stats['full_bju']} ({bju_stats['full_bju']/len(products)*100:.1f}%)")
        print(f"   ‚Ä¢ –•–æ—Ä–æ—à–µ–µ –ë–ñ–£ (3/4): {bju_stats['good_bju']} ({bju_stats['good_bju']/len(products)*100:.1f}%)")
        print(f"   ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {excellent_quality} ({excellent_quality/len(products)*100:.1f}%)")
        print(f"   ‚Ä¢ –ï—Å—Ç—å —Å–æ—Å—Ç–∞–≤: {composition_stats['has_composition']} ({composition_stats['has_composition']/len(products)*100:.1f}%)")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è: {duration/60:.1f} –º–∏–Ω—É—Ç")
        print(f"üíæ –§–∞–π–ª—ã: {csv_file}, {jsonl_file}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ —Ü–µ–ª–∏
        if len(products) >= 1200 and excellent_quality/len(products) >= 0.95:
            print("üéâ –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê: >1200 —Ç–æ–≤–∞—Ä–æ–≤ –∏ >95% –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö!")
        else:
            print(f"‚ö†Ô∏è  –¶–µ–ª—å –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞: –Ω—É–∂–Ω–æ >1200 —Ç–æ–≤–∞—Ä–æ–≤ (–µ—Å—Ç—å {len(products)}) –∏ >95% –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö (–µ—Å—Ç—å {excellent_quality/len(products)*100:.1f}%)")
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    finally:
        await antibot_client.close()


if __name__ == "__main__":
    asyncio.run(main())
